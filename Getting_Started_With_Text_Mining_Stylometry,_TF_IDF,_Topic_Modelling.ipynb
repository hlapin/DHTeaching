{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9q5iCpQK31i+k0cD5M3yd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hlapin/DHTeaching/blob/master/Getting_Started_With_Text_Mining_Stylometry%2C_TF_IDF%2C_Topic_Modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBagCN6tmFeW"
      },
      "source": [
        "# Introduction\n",
        "This colab notebook was prepared for teaching purposes for a session on text mining in a course on digital tools in historical research.  \n",
        "For data, the examples use the text of the *Federalist Papers*.   \n",
        "Much of what follows is derivative of resources available on the web, although I have revised material for display or pedagogical purposes. In particular I have made use of the tutorial on stylometry at _The Programing Historian_:\n",
        "> https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python   \n",
        "\n",
        "(The goal has been to provide \"plug-and-play\" demonstrations, leaving space and time open for discussion of methods and concepts.) \n",
        "For those of you new to Colab/Jupyter, there are text blocks, code blocks, and--after running code--output blocks. Subsections may be hidden by default, but `ctrl+[` should expand all sections and `ctrl+]` collapse them.    \n",
        "You should not need to write any code (although you are welcome to experiment!): just click the run (play arrow) button at the top left of each code block. You generally ***do*** need to run the code blocks in order.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjUGJpdg-Qi5"
      },
      "source": [
        "# Getting the text\n",
        "We are going to be working with the Federalist Papers  \n",
        "We need to:\n",
        "1. Download a zip file from github (Programming Historian repository)\n",
        "2. Unzip it and\n",
        "3. Unpack the files in a local [to Colab] folder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5VYgHAGFL4C"
      },
      "source": [
        "import requests, io, zipfile, os\n",
        "os.chdir ('/content/') # changes working directory on local machine\n",
        "r = requests.get('https://github.com/programminghistorian/ph-submissions/blob/gh-pages/assets/introduction-to-stylometry-with-python/stylometry-federalist.zip?raw=true')\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall() \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tc2wcWXMrZc"
      },
      "source": [
        "There is now a local file on the virtual machine you are using called `data` with the downloaded files.   \n",
        "You can check that in the files tools in the left hand panel:    \n",
        "> ![Screenshot 2023-04-05 131338.png](data:image/png;base64,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)    \n",
        "\n",
        "However, let's set the working directory to that directory and list its contents programmatically.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/data/')\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "d9FbWXA2KzkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0giYZI2WlYjN"
      },
      "source": [
        "## Text to Dataframe\n",
        "Now we are going to read each of the individual chapters into a table (a pandas dataframe, `dfPapers`) where each row is a document and that has the column headings `file_name` and `text`.\n",
        "\n",
        "For later use we are going to create a column `num` that has only the numerical part of the paper's name (`8` for `Federalist_8`) and we are going to extract an approximation of the document's title (the third line in each document; that will go into the column `title`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJWhHkFuRJNR"
      },
      "source": [
        "import pandas as pd # pandas is a data structure library\n",
        "import glob         # finds all pathnames matching a specific pattern\n",
        "\n",
        "\n",
        "# create an dataframe with each document as a row\n",
        "# we start by creating a \"dictionary\" to represent the columns\n",
        "results = {\"file_name\":[],\"num\":[],\"title\":[],\"text\":[]}\n",
        "\n",
        "for item in glob.glob('*[0-9].txt'):  # read only files with names ending \n",
        "                                      # with numerals.Why?\n",
        "   \n",
        "   # each `item` is a path that matches the pattern, e.g., 'Federalist_8.txt'\n",
        "\n",
        "   # below, the function split() splits a string into substrings based on \n",
        "   # a specified separator. \n",
        "   # [In Python, the first item in a list is indexed as 0 rather than 1]\n",
        "   \n",
        "   short = item.split('.')[0]           # grab filename without '.txt'\n",
        "   paperNum = int(short.split('_')[1])  # grab the numeral after '_'\n",
        "   with open(item, \"r\") as file_open:\n",
        "     txt = file_open.read()\n",
        "     results[\"file_name\"].append(short)\n",
        "     results[\"num\"].append(paperNum)\n",
        "     results[\"title\"].append(txt.split('\\n')[2])\n",
        "     results[\"text\"].append(txt.replace('\\n', ' '))\n",
        "\n",
        "# pandas has built in abilities to convert dictionaries to dataframes\n",
        "dfPapers = pd.DataFrame(results)\n",
        "dfPapers = dfPapers.sort_values([\"num\"])\n",
        "dfPapers = dfPapers.reset_index(drop=True)\n",
        "\n",
        "#let's check that we got the documents into shape\n",
        "dfPapers   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWozCPbBokdF"
      },
      "source": [
        "## A Bit of Cleanup\n",
        "Let's remove punctuation and convert all upper case to lower case, and then print a sample of our data to if we got it right.  \n",
        "> *Regular expressions* (often `regex`) refers to a set of operations on text that can be defined by patterns (a valid email address is an unbroken string, followed by '@' followed by a domain and one of a number of valid suffixes (.org, .edu, .ac.uk). For an example of how complex the regex may be to capture \"all\" emails see: https://regex101.com/library/8pGQaK\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yXp9DXZoXk8"
      },
      "source": [
        "import re #re is the module that does regular expression operations\n",
        "\n",
        "# note that pandas allows us to operate on all the cells in a column\n",
        "# of a dataframe by filtering by column label: dfPapers['text'] \n",
        "\n",
        "# regularize spacing: \n",
        "# replace one or more line breaks or spaces with single space\n",
        "dfPapers['text'] = dfPapers['text'].map(lambda x: \n",
        "                                    re.sub(r\"\\s+\", ' ', x))\n",
        "\n",
        "# remove punctuation, numerals, etc. This time replace with no space\n",
        "dfPapers['text'] = dfPapers['text'].map(lambda x: \n",
        "                                    re.sub(r\"[\\d\\'\\\"\\(\\)\\:;,\\.!?‘’“”]\", '', x))\n",
        "\n",
        "# convert characters to lower case\n",
        "dfPapers['text'] = dfPapers['text'].map(lambda x: x.lower())\n",
        "\n",
        "# again, let's check that we got the documents into proper shape\n",
        "dfPapers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMXhkotPt7Cj"
      },
      "source": [
        "# Some Exploratory Analysis\n",
        "First we are going to do some exploratory text analysis by making a word cloud.  \n",
        "How does the model select words to present?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd4b_a82y0xK"
      },
      "source": [
        "## Frequency vs significance\n",
        "In a word cloud, frequency determines the size of the word, but:\n",
        "* Is it really modelling the ***most frequent*** words?\n",
        "* What is the problem with words like may, will?\n",
        "\n",
        "The python library that does the work for us has a default set of `stopwords`: very common words that it filters out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-Sljwkkv5Vd"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# code adapted from https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
        "# See also: https://towardsdatascience.com/generate-meaningful-word-clouds-in-python-5b85f5668eeb\n",
        "\n",
        "# Join the different processed titles together into one long text.\n",
        "long_string = ','.join(list(dfPapers['text'].values))\n",
        "\n",
        "# Create a WordCloud object\n",
        "# You can change the parameters below\n",
        "wordcloud = WordCloud(background_color=\"white\", \n",
        "                        max_words=100, \n",
        "                        contour_width=3, \n",
        "                        contour_color='steelblue')\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chaZdyi_mPKt"
      },
      "source": [
        "## Let's run the same program without any stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGvLTzWjmORs"
      },
      "source": [
        "stopwords = set() # define the list of stopwords as an empty set \n",
        "\n",
        "# Join the different processed papers together into one long text.\n",
        "long_string = ','.join(list(dfPapers['text'].values))\n",
        "\n",
        "# Create a WordCloud object\n",
        "# You can change the parameters below\n",
        "wordcloud = WordCloud(background_color=\"white\", \n",
        "                        max_words=100, \n",
        "                        contour_width=3, \n",
        "                        stopwords = stopwords,\n",
        "                        collocations=False, #only single tokens\n",
        "                        contour_color='steelblue')\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQoinGp50cut"
      },
      "source": [
        "# Stylometry\n",
        "The approach we are taking to stylometry is based on most frequent (\"function\") words. We could refine this further, but the basic observation is that no two authors use very common words (in English) like \"the\" or \"and\" or \"of\" identically (or punctuate quite the same way). In principle, we should be able to create a \"fingerprint\" for a given author based on the distinctive use of these stopwords. \n",
        "From there techniques can become quite complicated, but the basic idea is that is that if we take a number of features (in our case, most frequent words) and see how these are used by each author, we can use this to measure distance between sample texts or authors' corpora. Before we do that we want to standardize our measurements (so that a larger corpus does not outweigh a smaller one) and decide on how to weight the features. (If the word \"the\" is about 10% of all the words it is clearly potentially distinctive; but how much should it count against the next most frequent words?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAC6Puo1X7a7"
      },
      "source": [
        "# Stylometry I Burrow's Delta and Author Identification\n",
        "> Adapted from \"Programming Historian\"   \n",
        "https://doi.org/10.46430/phen0078   \n",
        "\n",
        "This section uses the same texts as before (The Federalist Papers) to illustrate the use of **Burrows's Delta**. What this tries to do is to measure how each writer in a corpus uses \"function words\" (in English, the very common words like \"the\" and \"is\" and \"to\"; precisely the kinds of words that for other purposes would be classed as \"stopwords\").       \n",
        "`Delta` seeks to aggregate the observations about each of the features we are testing for (we are using the 30 most common words in the document set).  \n",
        "For each of the features we compare the frequency of the word in each of the texts we are examining in comparison with that of the corpus as a whole, and standardize the measurements across the corpus (so that more prolific authors [Hamilton] or more common words do not outweigh all the other authors or features).   \n",
        "We also hold out Federalist 64 as a test case, and calculate `Delta` between that essay, and the other authors. A smaller `Delta` between texts means that two texts are \"closer\" to each other. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-5h0QRzkYbG"
      },
      "source": [
        "## First let's modify the dataframe we created to add attribution\n",
        "Follows the canonical division plus test case as in PH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-VohefvPrSf"
      },
      "source": [
        "# A \"canonical\" division into authors plus one test case, as in PH\n",
        "papers = {\n",
        "    'Madison': [10, 14, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48],\n",
        "    'Hamilton': [1, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 21, 22, 23, 24,\n",
        "                 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 59, 60,\n",
        "                 61, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,\n",
        "                 78, 79, 80, 81, 82, 83, 84, 85],\n",
        "    'Jay': [2, 3, 4, 5],\n",
        "    'Shared': [18, 19, 20],\n",
        "    'Disputed': [49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 62, 63],\n",
        "    'TestCase': [64]\n",
        "}\n",
        "k, v = list(papers.keys()), list(papers.values())\n",
        "def return_attrib (num):\n",
        "  \"\"\" checks for the document no in lists of values\n",
        "      returns first letter of attribution\n",
        "      to use as label. \"\"\"\n",
        "  for i in v:\n",
        "    if num in i: \n",
        "      return k[v.index(i)][0]\n",
        "\n",
        "# insert attribution to datatable in first position if it does not exist\n",
        "# if not \"attrib\" in dfPapers.columns:\n",
        "#   dfPapers.insert(loc=0, column='attrib',value='')\n",
        "\n",
        "\n",
        "dfPapers[\"attrib\"] = dfPapers[\"num\"].apply(return_attrib)\n",
        "dfPapers.head(5)    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yww1zvE3krXZ"
      },
      "source": [
        "## Feature Selection\n",
        "We are choosing 30 as in PH example.\n",
        "\n",
        "Create a composite feature set for all texts (except test case). We will be using the `nltk` (Natural Language Toolkit) to do some of the calculating for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxNB8eixnBJ7"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# filter dataframe to exclude testcase\n",
        "# this is a variation of the operation we used above for the word cloud\n",
        "# create a single string of all the documents EXCLUDING the test case\n",
        "corpus = ' '.join(dfPapers[\"text\"][dfPapers[\"attrib\"] != \"T\"].values)\n",
        "\n",
        "# separate into a list of individual words ('tokens'). \n",
        "# These are our 'features'\n",
        "corpus_tokens = corpus.split()\n",
        "\n",
        "# create frequency list using built in nltk library `FreqDist`\n",
        "# this will give us the n (30) most common words and how often they appear\n",
        "# across our corpus\n",
        "whole_corpus_freq_dist = list(nltk.FreqDist(corpus_tokens).most_common(30))\n",
        "\n",
        "# # uncomment and run again to see the first 10\n",
        "# print(whole_corpus_freq_dist[ :10 ])\n",
        "\n",
        "# tabular data structure to contain our statistical information\n",
        "dfFeatures = pd.DataFrame( columns=[\"feats\"])\n",
        "dfFeatures[\"feats\"] = [w for w, freq in whole_corpus_freq_dist]\n",
        "dfFeatures[\"corpus\"] = [freq for w, freq in whole_corpus_freq_dist]\n",
        "\n",
        "# calculate frequency for each of the \"authors\"\n",
        "# authors to test\n",
        "authors = (\"H\",\"M\",\"J\",\"S\",\"D\",\"T\")\n",
        "for author in authors:\n",
        "  # in what follows we are repeating the same operation for each author\n",
        "  author_corpus = ' '.join(dfPapers[\"text\"][dfPapers[\"attrib\"] == author].values)\n",
        "\n",
        "  #separate into a list of values\n",
        "  author_tokens = author_corpus.split()\n",
        "\n",
        "  # get the total number of words for each author\n",
        "  # `len()` gives us the length (no. of items) in our list\n",
        "  author_length = len(author_tokens)\n",
        "\n",
        "  # copy the features from our datatable to a list\n",
        "  features = dfFeatures.feats.to_list()\n",
        "  \n",
        "  # The next line may be a little bit confusing because \n",
        "  #      *it is applying a function  based on each value in the `feats` column \n",
        "  #     * it also uses `list comprehension` as a shorthand.\n",
        "  # Essentialy, however, it does the following\n",
        "  # For the current author, for each feature (word), \n",
        "  # count the number of times it appears \n",
        "  # Calculate that number as the proportion to total author words\n",
        "  author_features = [author_tokens.count(x)/author_length for x in features]\n",
        "  \n",
        "  # append to df in a separate column for each author\n",
        "  dfFeatures[author] = author_features\n",
        "\n",
        "# the resulting table will give have:\n",
        "#      * `feats` column containing our thirty words\n",
        "#      * `corpus` column that counts the frequency of the word across the corpus\n",
        "#      * columns for each author giving the specific proportions for each feature \n",
        "\n",
        "# see the first five rows of this new table\n",
        "dfFeatures.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH3519LhH60C"
      },
      "source": [
        "## Means, Standard Deviation, z-scores\n",
        "\n",
        "The ***standard deviation*** is a measure of how widely a set of data varies from its mean value. (The standard deviation for the age of third graders will likely be much smaller than that of an entire elementary school.) So for each feature (word), we want to calculate how widely the values for each author vary from the mean.\n",
        "\n",
        "The ***z-score*** provides a standardized measurement (the number of standard deviations) of how far a measurement differs from the mean. The standardized measurement allows us to compare words with very different frequencies. (In our example, 'the' appears almost 18,000 times, but 'in' fewer than 4,500 times.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am-Ccdc1IbFg"
      },
      "source": [
        "import math\n",
        "# mean of the mean frequency of each feature\n",
        "# exclude testcase from means\n",
        "authors_no_T = [\"H\",\"M\", \"J\", \"S\", \"D\"]\n",
        "\n",
        "# calculate the means of columns. This will be column 8 below\n",
        "# pandas does this for us\n",
        "dfFeatures[\"means\"] = dfFeatures[authors_no_T].mean(axis=1)\n",
        "\n",
        "# calculate stdev of columns compared to sample\n",
        "# formula stdev = sum(sqrt((x[i] - x[sample])^2/(n - 1)))\n",
        "# There is a more efficient way of doing this in Pandas but\n",
        "# (a) I am a newbie\n",
        "# (b) this makes the process explicit\n",
        "\n",
        "n = len(authors_no_T)\n",
        "stdev = list([0]*len(features))\n",
        "\n",
        "for i in range(len(features)):\n",
        "  # since we are looping we want to reset some values to 0 each time we restart\n",
        "  squ_diff_fr_mean = 0\n",
        "  sum_squ_diff = 0\n",
        "\n",
        "  author_feature_values = dfFeatures.iloc[[i],[2,3,4,5,6]].values[0]\n",
        "  feature_mean = dfFeatures.iloc[[i],[8]].values[0]\n",
        "  \n",
        "  for j in range(len(authors_no_T)):\n",
        "    squ_diff_fr_mean = (author_feature_values[j] - feature_mean[0])**2\n",
        "    sum_squ_diff = sum_squ_diff + squ_diff_fr_mean\n",
        "    stdev[i] = math.sqrt(sum_squ_diff/(n - 1))\n",
        "\n",
        "dfFeatures[\"stdev\"] = stdev\n",
        "\n",
        "\n",
        "# create a new dataframe to hold z scores \n",
        "z_cols = list(authors)\n",
        "z_cols.extend([\"means\", \"stdev\"])\n",
        "dfZ = dfFeatures[z_cols].copy()\n",
        "\n",
        "#calcuate z-scores\n",
        "for author in authors:\n",
        "  dfZ[author] = (dfZ[author] - dfZ[\"means\"])/dfZ[\"stdev\"]\n",
        "dfZ.head(7)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmVv74MRxC4Z"
      },
      "source": [
        "Sample values in PH for T (Federalist 64) are:\n",
        "```\n",
        "Test case z-score for feature the is -0.7692828380408238\n",
        "Test case z-score for feature of is -1.8167784558461264\n",
        "Test case z-score for feature to is 1.032705844508835\n",
        "Test case z-score for feature and is 1.0268752924746058\n",
        "Test case z-score for feature in is 0.6085448501260903\n",
        "Test case z-score for feature a is -0.9341289591084886\n",
        "Test case z-score for feature be is 1.0279650702511498\n",
        "```\n",
        "Our values are close to those calculated there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHvhW9mBIaOY"
      },
      "source": [
        "## Calculate Burrows's Delta  \n",
        "> (from PH):  \n",
        "Finally, calculate a delta score comparing the anonymous paper with each candidate’s subcorpus. To do this, take the average of the ***absolute values of the differences between the z-scores for each feature between the anonymous paper and the candidate’s subcorpus.*** (Read that twice!) This [using the z-score (HL)] gives equal weight to each feature, no matter how often the words occur in the texts; otherwise, the top 3 or 4 features would overwhelm everything else.\n",
        "\n",
        "\n",
        "Taking the ***average*** of these values gives us a single statistic to compare for each author."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPc-Srze0bxh"
      },
      "source": [
        "# create a new data frame to hold delta\n",
        "dfDelta = dfZ.copy()\n",
        "col_keys = list(authors_no_T)\n",
        "col_vals = list(authors_no_T)\n",
        "for idx, v in enumerate(col_vals):\n",
        "  col_vals[idx] = \"T_to_\" + col_vals[idx]\n",
        "col_labels = dict(zip(col_keys, col_vals))\n",
        "dfDelta = dfDelta.rename(index=str, columns=col_labels)\n",
        "\n",
        "for v in col_vals:\n",
        "  dfDelta[v] = abs(dfDelta[v] - dfDelta[\"T\"])\n",
        "\n",
        "# add a row for sums and and delta and calculate\n",
        "\n",
        "dfDelta.loc[\"sum\"] = dfDelta.sum(axis=0)\n",
        "dfDelta.loc[\"delta\"] = dfDelta.loc[\"sum\"]/len(features)\n",
        "\n",
        "# clean up stupid error\n",
        "all_col_labels = list(dfDelta.columns)\n",
        "for col in all_col_labels:\n",
        "  if col not in col_vals:\n",
        "    dfDelta.loc[\"sum\",col] = \"\"\n",
        "    dfDelta.loc[\"delta\", col] = \"\"\n",
        "\n",
        "# show the last two lines of our table, with the sum and delta values\n",
        "dfDelta.tail(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lRGN-pryV6m"
      },
      "source": [
        "# report out\n",
        "\n",
        "for col in col_vals:\n",
        "  print(\"Delta for \" + col + \" is: \",dfDelta.loc[\"delta\",col])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY9YyB2tvoVp"
      },
      "source": [
        "This was the conclusion/outpt in PH\n",
        "```\n",
        "Delta score for candidate Hamilton is 1.768470453004334\n",
        "Delta score for candidate Madison is 1.6089724119682816\n",
        "**Delta score for candidate Jay is 1.5345768956569326**\n",
        "Delta score for candidate Disputed is 1.5371768107570636\n",
        "Delta score for candidate Shared is 1.846113566619675\n",
        "```\n",
        "\n",
        "This was Laramée's concluding paragraph:\n",
        "\n",
        "> As expected, Delta identifies John Jay as Federalist 64’s most likely author. It is interesting to note that, according to Delta, Federalist 64 is more similar to the disputed papers than to those known to have been written by Hamilton or by Madison; why that might be, however, is a question for another day.\n",
        "\n",
        "Has our work confirmed this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt8sCDu3IEms"
      },
      "source": [
        "# Stylometry II: Visualizing Similarity and Difference\n",
        "Here what we are doing, is essentially repeating what we did when applying Burrow's Delta, with two important changes: (1) we are applying it to each publication separately; and (2) we are comparing every work to every other work to gauge \"closeness\" or \"distance.\"   \n",
        "As with topic models or word vector embeddings (see below), we can think of each sample as a vector and use a distance measurement to position them in multidimensional space.\n",
        "Here there's a problem: how do we examine or visualize these relationships, and how do we find the important ones? (With 30 features, we need to allow up to 29 dimensions to describe the variation. Humans have difficulty imagining more than three.)   \n",
        "We are going to demo two techniques for visualizing multi-dimensional data: **Principal Component Analysis** (PCA) and **t-distributed stochastic neighbor embedding**.\n",
        "\n",
        "We will be using the Python library `sklearn` (sci-kit learn, developed for machine learning) to do the hard work for us, and matplotlib to do draw the graphs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   \n",
        "## PCA\n",
        "**Principal Component Analysis** attempts to calculate the directions of the dimensions and their weights. When we are looking at a two dimensional plot of principal components, we are looking at two specific dimensions calculated by the model. The analysis also can tell us how much of the variation among our test cases is explained by the specific components.   \n",
        "In our case, using the same number of features as before, the first two components account for a little under a quarter of the variation among the documents, but it looks like the first component (the horizontal or x axis) is where a good deal of the author differentiation is happening."
      ],
      "metadata": {
        "id": "FVUBMGG4xl9v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6dSr9dg8t1M"
      },
      "source": [
        "## Prepare the data as for Burrows' Delta\n",
        "Here we go over the steps that we followed for Burrow's delta to get a frequency table for each individual publication in the Federalist Papers.    \n",
        "We then reformat the data in a transposed table for PCA. (Without transposing, PCA would calculate how much features differ from one another over the 85D space of the authors. A very different question.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLloEreA8wno"
      },
      "source": [
        "# Repeat earlier steps, but now asigning each paper not author to a column \n",
        "# copy features and corpus data\n",
        "\n",
        "# repeating here allows us to experiment with feature values\n",
        "# see the explanations above\n",
        "\n",
        "corpus_tokens = corpus.split()\n",
        "\n",
        "# create frequency list using built in nltk function\n",
        "whole_corpus_freq_dist = list(nltk.FreqDist(corpus_tokens).most_common(30))\n",
        "\n",
        "# # uncomment and run again to see the check the first 10\n",
        "# print(whole_corpus_freq_dist[ :10 ])\n",
        "\n",
        "# data structure to contain our statistical information\n",
        "dfFeatures_pca = pd.DataFrame( columns=[\"feats\",\"corpus\"])\n",
        "dfFeatures_pca[\"feats\"] = [w for w, freq in whole_corpus_freq_dist]\n",
        "dfFeatures_pca[\"corpus\"] = [freq for w, freq in whole_corpus_freq_dist]\n",
        "\n",
        "# the last time we did this we bundled the docs together by attribution \n",
        "# This time, we create frequency table by publication rather than author\n",
        "\n",
        "for p in range(1,len(dfPapers)+1):  # iterate over all the files\n",
        "  paper_corpus = ' '.join(dfPapers[\"text\"][dfPapers[\"num\"] == p].values)\n",
        "\n",
        "  #separate into a list of tokens (features)\n",
        "  paper_tokens = paper_corpus.split()\n",
        "  \n",
        "  paper_length = len(paper_tokens)\n",
        "\n",
        "  # copy the features to a list\n",
        "  # for each feature count the proportion of features to total author words\n",
        "  # append to df\n",
        "\n",
        "  # create frequency list using built in nltk function\n",
        "  features = dfFeatures_pca.feats.to_list()\n",
        "\n",
        "  # calculate proportions\n",
        "  paper_features = [paper_tokens.count(x)/paper_length for x in features]\n",
        "  # and add this to the dataframe\n",
        "  dfFeatures_pca[p] = paper_features\n",
        "\n",
        "\n",
        "# transpose\n",
        "dfFeats_transp = dfFeatures_pca.transpose()\n",
        "dfFeats_transp.drop([\"feats\", \"corpus\"],inplace=True)\n",
        "\n",
        "for f in features:\n",
        "  dfFeats_transp.rename(columns={features.index(f):f},inplace=True)\n",
        "\t\n",
        "\n",
        "# # uncomment to show check data\n",
        "dfFeats_transp.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaVnZPyEcm3p"
      },
      "source": [
        "##Calcuate PCA \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsjFcX8Ocnf4"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "# # # to scale (mean = 0 total stdev = 1)\n",
        "# # # balances the weights of more or less frequent words\n",
        "# scaled = StandardScaler().fit_transform(dfFeats_transp)\n",
        "# principalComponents = pca.fit_transform(scaled)\n",
        "\n",
        "# to apply scaling uncomment above and comment next line\n",
        "principalComponents = pca.fit_transform(dfFeats_transp)\n",
        "principalDf = pd.DataFrame(data = principalComponents,\n",
        "                           columns = ['principal component 1', \n",
        "                           'principal component 2'])\n",
        "\n",
        "# add attrib labels\n",
        "principalDf[\"attrib\"] = dfPapers[\"attrib\"]\n",
        "\n",
        "principalDf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXJybJG4g5NG"
      },
      "source": [
        "## Plot a 2-Dimensional Grid for two Components.\n",
        "Plot a 2D scatter chart for the two first principal components.  \n",
        "Federalist 64 is marked in red on the plot. Does this confirm our earlier obsevation using Burrows's Delta?  \n",
        "This was Laramée's concluding paragraph:\n",
        ">As expected, Delta identifies John Jay as Federalist 64’s most likely author. It is interesting to note that, according to Delta, Federalist 64 is more similar to the disputed papers than to those known to have been written by Hamilton or by Madison; why that might be, however, is a question for another day.   \n",
        "\n",
        "Is this still true?  \n",
        "What happens when we increase the number of features?\n",
        "Decrease?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmuHWbJdg8Dp"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "## plotting code from:\n",
        "## https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
        "\n",
        "# print out the amount explained by the first two components\n",
        "print(pca.explained_variance_ratio_)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = [\"H\",\"M\",\"J\",\"S\",\"D\",\"T\"]\n",
        "colors = ['purple', 'green', 'black','yellow', 'blue', 'red']\n",
        "for target, color in zip(targets,colors):\n",
        "  indicesToKeep = principalDf['attrib'] == target\n",
        "  ax.scatter(principalDf.loc[indicesToKeep, 'principal component 1'],\n",
        "               principalDf.loc[indicesToKeep, 'principal component 2'],\n",
        "               c = color,\n",
        "               s = 50)\n",
        "ax.legend(targets)\n",
        "ax.grid()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## t-SNE\n",
        "**t-distributed stochastic neighbor embedding** is another widely used method for visualizing high-dimensional data in 2- to 3-dimensional space. \n",
        "It assigns a probability that each datapoint is similar to every other datapoint, and then attempts to map those relationships in a lower dimensional space. \n",
        "\n",
        "A frequent description is that PCA attempts to preserve the overall structure of the data as a whole, while t-SNE preserves the relationship with neighbors."
      ],
      "metadata": {
        "id": "GEQSsaPt0nbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating t-SNE\n",
        "Although the math of the two approaches is different, in scikitlearn, the procedure is more or less the same. \n",
        "\n",
        "Our data is the same `dFeats_transp` we created for PCA: a table that presents each of the words (features) as columns and each of the documents as rows.\n",
        "\n",
        "* set the parameters for the t-SNE model we are using\n",
        "\n",
        "* fit and transform our data to the model to generate an array of points\n",
        "\n",
        "* plot those points in 2-dimensional space\n"
      ],
      "metadata": {
        "id": "HPyidrrJmW2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the code here I have made use of the tutorial at:\n",
        "# https://github.com/olekscode/Examples-PCA-tSNE/blob/master/Python/Visualizing%20Iris%20Dataset%20using%20PCA%20and%20t-SNE.ipynb\n",
        "\n",
        "# We set the parameters for t-SNE\n",
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(n_components=2, \n",
        "            n_iter=1000, \n",
        "            random_state=0, \n",
        "            perplexity=15, \n",
        "            learning_rate='auto',\n",
        "            init='random')\n",
        "points = tsne.fit_transform(dfFeats_transp[features])"
      ],
      "metadata": {
        "id": "Pk63ZvYKCkAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsneDF = pd.DataFrame(data = points,\n",
        "                           columns = ['eigenvector 1', \n",
        "                           'eigenvector 2'])\n",
        "tsneDF[\"attrib\"] = dfPapers[\"attrib\"]\n",
        "tsneDF"
      ],
      "metadata": {
        "id": "XImziqmrFn9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig_tsne = plt.figure(figsize = (8,8))\n",
        "ax = fig_tsne.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Eigenvector 1', fontsize = 15)\n",
        "ax.set_ylabel('Eigenvector 2', fontsize = 15)\n",
        "ax.set_title('2 component t-SNE', fontsize = 20)\n",
        "targets = [\"H\",\"M\",\"J\",\"S\",\"D\",\"T\"]\n",
        "colors = ['purple', 'green', 'black','yellow', 'blue', 'red']\n",
        "for target, color in zip(targets,colors):\n",
        "  indicesToKeep = tsneDF['attrib'] == target\n",
        "  ax.scatter(tsneDF.loc[indicesToKeep, 'eigenvector 1'], \n",
        "             tsneDF.loc[indicesToKeep, 'eigenvector 2'],\n",
        "             c = color, \n",
        "             s = 50)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "metadata": {
        "id": "lsE21ejWDZM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic extraction: TF-IDF and LDA Topic Modelling"
      ],
      "metadata": {
        "id": "JMSQy3Hlroa6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF\n",
        "In our WordCloud experiment, we have already seen that term-frequency (how often a word appears) may not be the most helpful if we are trying to extract meaning. In that experiment we actively excluded stopwords, really frequent words, in order to get at words that are more indicative of what a text is about. **text frequency-inverse document freqency** (TF-IDF) is a method for weighting the \"important\" words in a given document. "
      ],
      "metadata": {
        "id": "_Eo1all5jBLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculting TF-IDF\n",
        "The intuition behind TF-IDF is pretty straightforward. In the sample of five titles from the Federalist Papers below, the word \"the\" is very common in general and it appears in every title at least once. This means that it is not likely to be a good indicator of what's in an individual document. However each title also has at least one word (e.g., `'foreign,' 'territory,' 'considered,' 'department,' 'senate'`) that appears in one document only. Most of these (maybe not 'considered', or 'method' in Federalist 49) may point to content. \n",
        "\n",
        "|index|file name|title|\n",
        "|---|---|---|\n",
        "|2|federalist\\_3|The Same Subject Continued \\(Concerning Dangers From Foreign Force and|\n",
        "|13|federalist\\_14|Objections to the Proposed Constitution From Extent of Territory|\n",
        "|41|federalist\\_42|The Powers Conferred by the Constitution Further Considered|\n",
        "|48|federalist\\_49|Method of Guarding Against the Encroachments of Any One Department of|\n",
        "|63|federalist\\_64|The Powers of the Senate|\n",
        "\n",
        "In the title to Federalist 2 the **term frequency** of 'the' and 'foreign' are the same: 1 out of 10 or 0.1\n",
        "\n",
        "**document frequency** refers to the proportion of documents in which the word appears. For 'the' that is 5/5 or 1. For 'foreign' in the title of Federalist 2 the document frequency is 1/5 or 0.2\n",
        "\n",
        "**In principle**, for any given term in  any given document we get the TF-IDF score by dividing  the term frequency by the document frequency (multiplying it by the inverse of the document frequency). For 'territory' in the title of Federalist 2, TF-IDF is  0, since the term frequency of 'territory' in that text is 0:\n",
        "\n",
        ">TF-IDF = 0/0.2 = 0\n",
        "\n",
        "The word  'the' has a TF-IDF of:\n",
        "\n",
        ">TF-IDF = 0.1/1 = 0.1\n",
        "\n",
        "For 'foreign':\n",
        ">TF-IDF = 0.1/0.2 = 0.5\n",
        "\n",
        "By taking into account the document frequency of 'the' and 'foreign' we count 'foreign' as five times as \"important\" for this text as 'the.'\n",
        "\n",
        "**In practice**, TF-IDF uses a formula based on the natural logarithm of the document frequency to calculate the IDF.    \n",
        "    \n",
        "The next code block uses Python's `sklearn` library to calculate TF-IDF"
      ],
      "metadata": {
        "id": "Wn3l-b4RVQ2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# the n most frequent features (words) across the corpus\n",
        "# experiment with different values and see the results in the next cell\n",
        "\n",
        "n = 1000\n",
        "\n",
        "# set up the constraints for the structure that will\n",
        "# represent our text as mathematical vectors\n",
        "tfidf = TfidfVectorizer(\n",
        "    min_df = 5,             # ONLY select words appearing in more than 5 docs\n",
        "    max_df = 0.95,          # DO NOT words appearing in >= 95% of docs\n",
        "    max_features = n,       # the n most frequent words across the corpus\n",
        "    stop_words = 'english'  # remove very common english words\n",
        ")\n",
        "\n",
        "# calculate the tf-idf values for each word in all the documents\n",
        "# As before, we `fit` and `transform` the data to calculate the TF-IDF values.\n",
        "# For convenience, here we do this in two separate operations.\n",
        "\n",
        "# `fit` calculates the df and idf values\n",
        "# and sets the parameters to create a standardized array\n",
        "tfidf.fit(dfPapers.text)"
      ],
      "metadata": {
        "id": "0yPV4Lr0N7Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell to get the first 100 feature names (words) \n",
        "# based on different values of n in the preceding cell\n",
        "tfidf.get_feature_names_out()[:100]"
      ],
      "metadata": {
        "id": "erT_xUgtFQYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a DataFrame in which every row is a document, and the columns give us the TF-IDF score for each word. "
      ],
      "metadata": {
        "id": "mFLbLBSikjx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform creates the array of values for each feature for each document\n",
        "text = tfidf.transform(dfPapers.text)\n",
        "\n",
        "# sklearn produces a \"sparse array,\" that skips over zero values to save memory\n",
        "# For Pandas to work with the array we need to convert it to a dense array \n",
        "# that allows us to address every value by row and column.\n",
        "dfTFIDF = pd.DataFrame(text.todense())\n",
        "\n",
        "# get the names of the columns (= words) from our `fitting` operation\n",
        "dfTFIDF.columns = tfidf.get_feature_names_out()\n",
        "\n",
        "# view a snapshot of the new dataframe\n",
        "dfTFIDF"
      ],
      "metadata": {
        "id": "AZ7gP6AaBXM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examining Results\n",
        "What are the ten words with the highest TF-IDF score for each document? (Again, you can experiment with larger or smaller numbers.)    \n",
        "Impressionistically, how good is the fit between the document titles (an author's or editor's idea of what the work is about) and the TF-IDF keywords?"
      ],
      "metadata": {
        "id": "mkQFe2XWIA1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10    # the number of key words we want to return\n",
        "\n",
        "# The Pandas function `nlargest()` applied to each row (`axis=1`)\n",
        "# gives us the top n words\n",
        "dfPapers['tfidf_keywords'] = dfTFIDF.apply(lambda x: \n",
        "                                      ', '.join(x.nlargest(n).index.tolist()), \n",
        "                                      axis=1)\n",
        "dfPapers[['num','title','tfidf_keywords']]"
      ],
      "metadata": {
        "id": "Bi9D_LeFykCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA topic modeling\n",
        "**Topic modeling** means creating a model that describes what topics are in a corpus, how the topics are related to individual texts and to the corpus as a whole.  \n",
        "**LDA** or Latent Dirichlet Allocation is a very widely used form of topic modeling. Where with TF-IDF we calculated a score for each feature (word) across the documents algebraically, LDA is a probabilistic model. Given a set of documents and a specified number of topics, over many iterations it calculates the probability that a given word belongs with any specific topic and that a given topic belongs with any specific document.\n",
        "\n",
        "We are using LDA to determine the topics that are operative in our corpus. In practice, we could use the results to cluster our documents by likely topic or assign topics to a previously unseen document. \n",
        "         \n",
        "This part of the tutorial has adapted code from: Karsdorp, Folgert, Mike Kestemont, and Allen Riddell. _Data Analysis: Case Studies with Python_. Princeton: Princeton University Press, 2021.\n",
        "See also: https://www.humanitiesdataanalysis.org/topic-models/notebook.html"
      ],
      "metadata": {
        "id": "wThaZ07yp0Mo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating LDA\n",
        "We use `sklearn` to do the actual calculating and modeling.   \n",
        "First, represent the text as a matrix where each row corresponds to a document and each column gives the count of a term in the corpus. (This is analogous to calculating the term frequency for TF-IDF.)    \n",
        "As with TF-IDF, we can filter out very frequent or infrequent words."
      ],
      "metadata": {
        "id": "KVEGhZ2UJEPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.feature_extraction.text as text\n",
        "\n",
        "# Use sklearn's feature_extraction module to represent our texts with numerals\n",
        "# and create a document-term matrix (`dtm`).\n",
        "\n",
        "# set min_df and max_df; compare above for TF-IDF \n",
        "# when set as an integer, a count; as a decimal, a proportion \n",
        "vec = text.CountVectorizer(min_df=5, \n",
        "                           max_df=0.5, \n",
        "                           stop_words='english')\n",
        "dtm = vec.fit_transform(dfPapers['text'])\n",
        "\n",
        "print(f'Shape of document-term matrix: {dtm.shape}. '\n",
        "      f'Number of tokens {dtm.sum()}')"
      ],
      "metadata": {
        "id": "pmhmJTlDp8pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a model (with 10 components or \"topics\") and use `sklearn`'s `LatentDirichletAllocation` class to do the model fitting."
      ],
      "metadata": {
        "id": "s_YqnI1M-ZNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.decomposition as decomposition\n",
        "\n",
        "# parameter estimation: using sklearn's default estimation method \n",
        "# in the class LatentDirichletAllocation: variational inference\n",
        "\n",
        "# DEPENDING ON THE CORPUS SIZE THIS CAN TAKE A WHILE\n",
        "\n",
        "# define model\n",
        "model = decomposition.LatentDirichletAllocation(\n",
        "    n_components=10, learning_method='online', random_state=1)\n",
        "\n",
        "document_topic_distributions = model.fit_transform(dtm)"
      ],
      "metadata": {
        "id": "HmiI463ACYPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get array of \"columns\" (unique words) in our model\n",
        "# These should be identical with the terms we extracted above  \n",
        "vocabulary = vec.get_feature_names_out()"
      ],
      "metadata": {
        "id": "LugbmltjCq7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell to see these column headings\n",
        "vocabulary"
      ],
      "metadata": {
        "id": "T2YIrMSeOmaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the model created by `sklearn` extract the topic-word-distributions and the document-topic distributions and store them in two tables (DataFrames). _Topic-word-distributions_ refers to the probabilities that specific words are associated with a given topic; _Document-topic-distributions_ to the probabilities that a certain topic is associated with a given document. "
      ],
      "metadata": {
        "id": "afUgdfBvf8B_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For convenience label the topics Topic 1, Topic 2, etc. \n",
        "# We need to add one to each current topic number for 1 indexing rather than 0.\n",
        "\n",
        "topic_names = [f'Topic {str(k + 1)}' for k in range(10)]\n",
        "\n",
        "# save topic word distributions and document topic distributions in separate dfs\n",
        "\n",
        "# topic word distributions shows how much words are associated with each topic\n",
        "topic_word_distributions = pd.DataFrame(\n",
        "    model.components_, columns=vocabulary, index=topic_names)\n",
        "\n",
        "# document topic distributions shows how much topics \n",
        "# are associated with each document\n",
        "document_topic_distributions = pd.DataFrame(\n",
        "    document_topic_distributions, columns=topic_names, index=dfPapers.index)\n",
        "\n",
        "# The next cells show what these two dataframes look like"
      ],
      "metadata": {
        "id": "IB5o0ue_DAp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Topic word distributions:***   \n",
        "For each topic (row), give the probability of each word (column) being associated with that topic"
      ],
      "metadata": {
        "id": "LZ8Ep1p2xN4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_word_distributions"
      ],
      "metadata": {
        "id": "uGxdkwMExZAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Document topic distributions:***    \n",
        "For each document (row), give the probability that any given topic (column) is associated with that document:"
      ],
      "metadata": {
        "id": "TtuERj8jxR5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_topic_distributions"
      ],
      "metadata": {
        "id": "Eds1u43oxyFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the results of our LDA model"
      ],
      "metadata": {
        "id": "YxNunJEbJbSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the document topic distribution table to find the top topics associated with any given document. This example uses  Federalist 12 (row 11; Python starts lists at 0). You can change the number to test other documents."
      ],
      "metadata": {
        "id": "oZ_fWEs_zizV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_num = 11\n",
        "\n",
        "# unsorted:\n",
        "print(f'Topics in Federalist {doc_num + 1}')\n",
        "print(document_topic_distributions.loc[doc_num])\n",
        "\n",
        "# sorted:\n",
        "print(f\"\\nTopics in Federalist {doc_num + 1}, Sorted\")\n",
        "topic_dist_test = document_topic_distributions.loc[doc_num]\n",
        "topic_dist_test.sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "4BT-vWPOPfVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also transpose (`.T`) the table to show how documents are associated with individual topics. This cell extracts the document (by column no.) with the highest probability."
      ],
      "metadata": {
        "id": "RmBgurXz1dRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# documents most strongly associated with each topic\n",
        "d = document_topic_distributions.T.eq(document_topic_distributions.T.max(\n",
        "                                      axis=1),axis=0)\n",
        "d.dot(d.columns)"
      ],
      "metadata": {
        "id": "kAkUjhItdV_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the topic word distributions dataframe we created earlier, we can make a table that presents the top `n` words (we are using ten; again, you can experiment) associated with each topic.   \n",
        "This is usually a more informative way of thinking about our topics than just the numerical label."
      ],
      "metadata": {
        "id": "pGF7DRqV9YlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create df with topic names as index and joined top n words as content\n",
        "# function to get the top n words sorted\n",
        "def word_dist_for_topic(topic,n):\n",
        "    \"\"\"\n",
        "    returns the top n words for specified topic\n",
        "    \"\"\"\n",
        "    twd = topic_word_distributions.loc[topic].sort_values(ascending=False).head(n)\n",
        "    return twd\n",
        "    \n",
        "n = 10 # number of words to return\n",
        "\n",
        "\n",
        "topics_dict = dict()\n",
        "for t in topic_names:\n",
        "    topic_words = word_dist_for_topic(t,n)\n",
        "    topic_top_words_joined = \", \".join(topic_words.index)\n",
        "    topics_dict[t]=topic_top_words_joined\n",
        "\n",
        "topics_df = pd.DataFrame(topics_dict.items(),columns=['topic','topic words'])  \n",
        "topics_df.set_index('topic')\n",
        "\n",
        "# display topics and most strongly associated topic words\n",
        "print (\"Topics and Topic Words\")\n",
        "topics_df"
      ],
      "metadata": {
        "id": "yEDCEpfULc1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With these pieces in place, we can create a table that shows, for each document (row), the most strongly associated topic (by label) and the corresponding topic words. \n"
      ],
      "metadata": {
        "id": "mdbiXH6QtDps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# idxmax will give us the column with the highest value in each row\n",
        "df = pd.DataFrame(document_topic_distributions.idxmax(axis=1))\n",
        "\n",
        "# provide a name for the topic column\n",
        "df.rename(columns={0: 'topic'},inplace=True)\n",
        "\n",
        "\n",
        "df['lda_keywords'] = df['topic'].apply(lambda x : \n",
        "                                 topics_df.loc[topics_df['topic'] == x,\n",
        "                                'topic words'].item())\n",
        "\n",
        "# view a snapshot of the table.\n",
        "df"
      ],
      "metadata": {
        "id": "lNC-ViJ8f7j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing the results of TF-IDF and LDA\n",
        "If you've done this tutorial straight through, you can now compare the results of TF-IDF and LDA on each of the Federalist Papers."
      ],
      "metadata": {
        "id": "p1n_C0XZnCBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing TFIDF results and LDA.\n",
        "dfPapers.join(df)[['num','title','tfidf_keywords','topic','lda_keywords']]"
      ],
      "metadata": {
        "id": "56MTBptvPbmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Word About Vector Space\n",
        "For \"Burrow's Delta\" we calculated a single statistic for each author that combined our measurements for each feature.    \n",
        "In our experiments with PCA and t-SNE, we did something else: we represented each document as a ***vector***, a numerical expression with values for each feature.        \n",
        "TF-IDF is a method for calculating and weighting the values individual features will have.  \n",
        "In LDA, a \"topic\" is a vector with values (probabilities) that any of the words in our model is associated with that topic. A \"document\" is represented as a vector with values across all the topics.   \n",
        "The number of features that we use dictates the number of dimensions that we need to express our vector in a multi-dimensional space. The mathematics of vectors in multi-dimensional space is central to machine learning and AI methods. A standard task in interpeting results in complex vector space  is to reduce (and reconfigure) the number of dimensions to a number (typically two or three, which humans can imagine in visually) along which the most important differences or similarities can be measured. t-SNE and PCA are methods for doing precisely this. LDA is also effectively a type of dimension reduction, limiting them to the number of topics.  \n",
        "The Python library [pyLDAvis]('https://pyldavis.readthedocs.io/en/latest/readme.html') provides tools to visualize the results of LDA in two dimensional space."
      ],
      "metadata": {
        "id": "57a6TTlzta6K"
      }
    }
  ]
}