{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMt2P1bl6BYCoqHAzxLWLeO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hlapin/DHTeaching/blob/master/Getting_Started_With_Text_Mining_Stylometry%2C_TF_IDF%2C_Topic_Modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBagCN6tmFeW"
      },
      "source": [
        "# Introduction\n",
        "This colab notebook was prepared for teaching purposes for a session on text mining in a course on digital tools in historical research.  \n",
        "As a text source the examples use the text of the *Federalist Papers*.   \n",
        "Much of what follows is derivative of resources available on the web, although I have revised material for display or pedagogical purposes. My dependence is especially visible with the tutorial on stylometry at _The Programing Historian_:\n",
        "> https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python   \n",
        "\n",
        "(The goal has been to provide \"plug-and-play\" demonstrations, leaving space open for discussion of methods and concepts.) \n",
        "For those of you new to Colab/Jupyter, there are text blocks, code blocks, and--after running code--output blocks. Subsections may be hidden by default, but `ctrl+[` should expand all sections and `ctrl+]` collapse them.    \n",
        "You should not need to write any code (although you are welcome to experiment): just click the run (play arrow) button at the top left of each code block. You generally ***do*** need to run the code blocks in order.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjUGJpdg-Qi5"
      },
      "source": [
        "# Getting the text\n",
        "We are going to be working with the Federalist Papers  \n",
        "We need to:\n",
        "1. Download a zip file from github (Programming Historian repository)\n",
        "2. Unzip it and\n",
        "3. Unpack the files in a local folder [local to Colab]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5VYgHAGFL4C"
      },
      "source": [
        "import requests, io, zipfile, os\n",
        "os.chdir ('/content/') # changes working directory on local machine\n",
        "r = requests.get('https://github.com/programminghistorian/ph-submissions/blob/gh-pages/assets/introduction-to-stylometry-with-python/stylometry-federalist.zip?raw=true')\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall() \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tc2wcWXMrZc"
      },
      "source": [
        "There is now a local file on the virtual machine you are using called `data` with the files   \n",
        "You can check that in the files tools in the left hand panel.   \n",
        "However, let's set the working directory to that directory confirm it and list its contents programmatically.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/data/')\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "d9FbWXA2KzkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0giYZI2WlYjN"
      },
      "source": [
        "## Text to Dataframe\n",
        "Now we are going to read each of the individual chapters into a table (a pandas dataframe, `dfPapers`) where each row is a document and that has the column headings `file_name` and `text`.\n",
        "\n",
        "For later use we are going to create a column `num` that has only the numerical part of the paper's name (`8` for `Federalist_8`) and we are going to extract an approximation of the document's title (the third line in each document; that will go into `title`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJWhHkFuRJNR"
      },
      "source": [
        "import pandas as pd # pandas is a data structure library\n",
        "import glob         # finds all pathnames matching a specific pattern\n",
        "\n",
        "\n",
        "# create an dataframe with each document as a row\n",
        "# we start by creating a \"dictionary\" to represent the columns\n",
        "results = {\"file_name\":[],\"num\":[],\"title\":[],\"text\":[]}\n",
        "\n",
        "for item in glob.glob('*[0-9].txt'):  # read only files with names ending \n",
        "                                      # with numerals.Why?\n",
        "   \n",
        "   # each `item` is a path that matches the pattern, e.g., 'Federalist_8.txt'\n",
        "\n",
        "   # below, the function split() splits a string into substrings based on \n",
        "   # a specified separator. \n",
        "   # [In Python, the first item in a list is indexed as 0 rather than 1]\n",
        "   \n",
        "   short = item.split('.')[0]           # grab filename without '.txt'\n",
        "   paperNum = int(short.split('_')[1])  # grab the numeral after '_'\n",
        "   with open(item, \"r\") as file_open:\n",
        "     txt = file_open.read()\n",
        "     results[\"file_name\"].append(short)\n",
        "     results[\"num\"].append(paperNum)\n",
        "     results[\"title\"].append(txt.split('\\n')[2])\n",
        "     results[\"text\"].append(txt.replace('\\n', ' '))\n",
        "\n",
        "# pandas has built in abilities to convert dictionaries to dataframes\n",
        "dfPapers = pd.DataFrame(results)\n",
        "dfPapers = dfPapers.sort_values([\"num\"])\n",
        "dfPapers = dfPapers.reset_index(drop=True)\n",
        "\n",
        "#let's check that we got the documents into shape\n",
        "dfPapers   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWozCPbBokdF"
      },
      "source": [
        "## A Bit of Cleanup\n",
        "Let's remove punctuation and convert all upper case to lower case, and then print a sample of our data to if we got it right.  \n",
        "> *Regular expressions* (often `regex`) refers to a set of operations on text that can be defined by patterns (a valid email address is an unbroken string, followed by '@' followed by a domain and one of a number of valid suffixes (.org, .edu, .ac.uk). For an example of how complex the regex may be to capture \"all\" emails see: http://emailregex.com/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yXp9DXZoXk8"
      },
      "source": [
        "import re #re is the module that does regular expression operations\n",
        "\n",
        "# note that pandas allows us to operate on all the cells in a column\n",
        "# of a dataframe by filtering by column label: dfPapers['text'] \n",
        "\n",
        "# regularize spacing: \n",
        "# replace one or more line breaks or spaces with single space\n",
        "dfPapers['text'] = dfPapers['text'].map(lambda x: re.sub(r\"\\s+\", ' ', x))\n",
        "\n",
        "# remove punctuation, numerals, etc. This time replace with no space\n",
        "dfPapers['text'] = dfPapers['text'].map(lambda x: re.sub(r\"[\\d\\'\\\"\\(\\)\\:;,\\.!?‘’“”]\", '', x))\n",
        "\n",
        "# convert characters to lower case\n",
        "dfPapers['text'] = dfPapers['text'].map(lambda x: x.lower())\n",
        "\n",
        "# again, let's check that we got the documents into proper shape\n",
        "dfPapers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMXhkotPt7Cj"
      },
      "source": [
        "# Some Exploratory Analysis\n",
        "First we are going to do some exploratory text analysis by making a word cloud.  \n",
        "How does the model select words to present?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd4b_a82y0xK"
      },
      "source": [
        "## Frequency vs significance\n",
        "In a word cloud, frequency determines the size of the word, but:\n",
        "* Is it really modelling the ***most frequent*** words?\n",
        "* What is the problem with words like may, will?\n",
        "\n",
        "The python library that does the work for us has a default set of `stopwords`: very common words that it filters out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-Sljwkkv5Vd"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# code adapted from https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
        "# See also: https://towardsdatascience.com/generate-meaningful-word-clouds-in-python-5b85f5668eeb\n",
        "\n",
        "# Join the different processed titles together into one long text.\n",
        "long_string = ','.join(list(dfPapers['text'].values))\n",
        "\n",
        "# Create a WordCloud object\n",
        "# You can change the parameters below\n",
        "wordcloud = WordCloud(background_color=\"white\", \n",
        "                        max_words=100, \n",
        "                        contour_width=3, \n",
        "                        contour_color='steelblue')\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chaZdyi_mPKt"
      },
      "source": [
        "## Let's run the same function without any stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGvLTzWjmORs"
      },
      "source": [
        "stopwords = set() # define the list of stopwords as an empty set \n",
        "\n",
        "# Join the different processed papers together into one long text.\n",
        "long_string = ','.join(list(dfPapers['text'].values))\n",
        "\n",
        "# Create a WordCloud object\n",
        "# You can change the parameters below\n",
        "wordcloud = WordCloud(background_color=\"white\", \n",
        "                        max_words=100, \n",
        "                        contour_width=3, \n",
        "                        stopwords = stopwords,\n",
        "                        collocations=False, #only single tokens\n",
        "                        contour_color='steelblue')\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQoinGp50cut"
      },
      "source": [
        "# Stylometry\n",
        "The approach we are taking to stylometry is based on most frequent (\"function\") words. We could refine this further, but the basic observation is that no two authors use very common words (in English) like \"the\" or \"and\" or \"of\" identically (or punctuate quite the same way). In priniple, we should be able to create a \"fingerprint\" for a given author based on the distinctive use of these stopwords. \n",
        "From there techniques can become quite complicated, but the basic idea is that is that if we take a number of features (in our case, most frequent words) and see how these are used by each author, we can use this to measure distance between sample texts or authors' corpora. Before we do that we want to standardize our measurements (so that a larger corpus does not outweigh a smaller one) and decide on how to weight the features. (If the word \"the\" is about 10% of all the words it is clearly potentially distinctive; but how much should it count against the next most frequent words?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAC6Puo1X7a7"
      },
      "source": [
        "# Stylometry I Author Identification\n",
        "> Adapted from \"Programming Historian\"   \n",
        "https://doi.org/10.46430/phen0078   \n",
        "\n",
        "This section uses the same texts as before (the Federalist Papers) to illustrate the use of **Burrows's Delta**. What this tries to do is to measure how each writer in a corpus uses \"function words\" (in English, the very common words like \"the\" and \"is\" and \"to\").       \n",
        "`Delta` seeks to aggregate the observations about each of the features we are testing for (we have used the 30 most common words in the document set).  \n",
        "For each of the features we compare the frequency of the word in each of the texts we are examining in comparison with that of the corpus as a whole, and standardize the measurements across the corpus (so that more prolific authors [Hamilton] or more common words do not outweigh all the other authors or features).   \n",
        "We also hold out Federalist 64 as a test case, and calculate `Delta` between that essay, and the other authors. A smaller `Delta` between texts means that two texts are \"closer\" to each other. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-5h0QRzkYbG"
      },
      "source": [
        "## First let's modify the dataframe we created to add attribution\n",
        "Follows the canonical division plus test case as in PH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-VohefvPrSf"
      },
      "source": [
        "import nltk\n",
        "\n",
        "%unload_ext google.colab.data_table\n",
        "# A \"canonical\" division into authors plus one test case, as in PH\n",
        "papers = {\n",
        "    'Madison': [10, 14, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48],\n",
        "    'Hamilton': [1, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 21, 22, 23, 24,\n",
        "                 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 59, 60,\n",
        "                 61, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,\n",
        "                 78, 79, 80, 81, 82, 83, 84, 85],\n",
        "    'Jay': [2, 3, 4, 5],\n",
        "    'Shared': [18, 19, 20],\n",
        "    'Disputed': [49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 62, 63],\n",
        "    'TestCase': [64]\n",
        "}\n",
        "k, v = list(papers.keys()), list(papers.values())\n",
        "def return_attrib (num):\n",
        "  \"\"\" checks for the document no in lists of values\n",
        "      returns first letter of attribution\n",
        "      to use as label. \"\"\"\n",
        "  for i in v:\n",
        "    if num in i: \n",
        "      return k[v.index(i)][0]\n",
        "\n",
        "# insert attribution to datatable in first position if it does not exist\n",
        "# if not \"attrib\" in dfPapers.columns:\n",
        "#   dfPapers.insert(loc=0, column='attrib',value='')\n",
        "\n",
        "\n",
        "dfPapers[\"attrib\"] = dfPapers[\"num\"].apply(return_attrib)\n",
        "dfPapers.head(5)    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yww1zvE3krXZ"
      },
      "source": [
        "## Feature Selection\n",
        "We are choosing 30 as in PH example.\n",
        "\n",
        "Create a composite feature set for all texts (except test case)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxNB8eixnBJ7"
      },
      "source": [
        "# filter dataframe to exclude testcase\n",
        "# this is a variation of the operation we used above for the word cloud\n",
        "# create a single string\n",
        "corpus = ' '.join(dfPapers[\"text\"][dfPapers[\"attrib\"] != \"T\"].values)\n",
        "\n",
        "# separate into a list of individual words ('tokens'). \n",
        "# These are our 'features'\n",
        "corpus_tokens = corpus.split()\n",
        "\n",
        "# create frequency list using built in nltk library\n",
        "# this will give us the n (30) most common words and how often they appear\n",
        "whole_corpus_freq_dist = list(nltk.FreqDist(corpus_tokens).most_common(30))\n",
        "\n",
        "# # uncomment and run again to see the first 10\n",
        "# print(whole_corpus_freq_dist[ :10 ])\n",
        "\n",
        "# data structure to contain our statistical information\n",
        "dfFeatures = pd.DataFrame( columns=[\"feats\"])\n",
        "dfFeatures[\"feats\"] = [w for w, freq in whole_corpus_freq_dist]\n",
        "dfFeatures[\"corpus\"] = [freq for w, freq in whole_corpus_freq_dist]\n",
        "\n",
        "# calculate frequency for each of the \"authors\"\n",
        "# authors to test\n",
        "authors = (\"H\",\"M\",\"J\",\"S\",\"D\",\"T\")\n",
        "for author in authors:\n",
        "  author_corpus = ' '.join(dfPapers[\"text\"][dfPapers[\"attrib\"] == author].values)\n",
        "\n",
        "  #separate into a list of values\n",
        "  author_tokens = author_corpus.split()\n",
        "\n",
        "  # create frequency list using built in nltk function\n",
        "  author_length = len(author_tokens)\n",
        "\n",
        "  # copy the features to a list\n",
        "  # for each feature count the proportion of features to total author words\n",
        "  # append to df\n",
        "\n",
        "  features = dfFeatures.feats.to_list()\n",
        "  author_features = [author_tokens.count(x)/author_length for x in features]\n",
        "  # dfFeatures[author] = author_features\n",
        "  dfFeatures[author] = author_features\n",
        "\n",
        "dfFeatures.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH3519LhH60C"
      },
      "source": [
        "## Means, Standard Deviation, z-scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am-Ccdc1IbFg"
      },
      "source": [
        "import math\n",
        "# mean of the mean frequency of each feature\n",
        "# exclude testcase from means\n",
        "authors_no_T = [\"H\",\"M\", \"J\", \"S\", \"D\"]\n",
        "\n",
        "#calculate the means of columns\n",
        "dfFeatures[\"means\"] = dfFeatures[authors_no_T].mean(axis=1)\n",
        "\n",
        "# dfFeatures\n",
        "\n",
        "# calculate stdev of columns compared tos sample\n",
        "# formula stdev = sum(sqrt((x[i] - x[sample])^2/(n - 1)))\n",
        "# should be a more efficient way of doing this in Pandas but\n",
        "# (a) I am a newbie\n",
        "# (b) this makes the process explicit\n",
        "\n",
        "n = len(authors_no_T)\n",
        "stdev = list([0]*len(features))\n",
        "\n",
        "for i in range(len(features)):\n",
        "  squ_diff_fr_mean = 0\n",
        "  sum_squ_diff = 0\n",
        "  author_feature_values = dfFeatures.iloc[[i],[2,3,4,5,6]].values[0]\n",
        "  feature_mean = dfFeatures.iloc[[i],[8]].values[0]\n",
        "  \n",
        "  for j in range(len(authors_no_T)):\n",
        "    squ_diff_fr_mean = (author_feature_values[j] - feature_mean[0])**2\n",
        "    sum_squ_diff = sum_squ_diff + squ_diff_fr_mean\n",
        "    stdev[i] = math.sqrt(sum_squ_diff/(n - 1))\n",
        "\n",
        "dfFeatures[\"stdev\"] = stdev\n",
        "\n",
        "# z-scores\n",
        "# formula z[j] = (Observed[j] - mean[j])/stdev[j]\n",
        "\n",
        "# dataframe to hold z scores\n",
        "z_cols = list(authors)\n",
        "z_cols.extend([\"means\", \"stdev\"])\n",
        "#z_cols\n",
        "#calcuate z-scores\n",
        "dfZ = dfFeatures[z_cols].copy()\n",
        "for author in authors:\n",
        "  dfZ[author] = (dfZ[author] - dfZ[\"means\"])/dfZ[\"stdev\"]\n",
        "dfZ.head(7)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmVv74MRxC4Z"
      },
      "source": [
        "Sample values in PH for T (Federalist 64) are:\n",
        "```\n",
        "Test case z-score for feature the is -0.7692828380408238\n",
        "Test case z-score for feature of is -1.8167784558461264\n",
        "Test case z-score for feature to is 1.032705844508835\n",
        "Test case z-score for feature and is 1.0268752924746058\n",
        "Test case z-score for feature in is 0.6085448501260903\n",
        "Test case z-score for feature a is -0.9341289591084886\n",
        "Test case z-score for feature be is 1.0279650702511498\n",
        "```\n",
        "Our values are close to those calculated there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHvhW9mBIaOY"
      },
      "source": [
        "## Calculate Burrows's Delta  \n",
        "> (from PH)  \n",
        "Finally, calculate a delta score comparing the anonymous paper with each candidate’s subcorpus. To do this, take the average of the ***absolute values of the differences between the z-scores for each feature between the anonymous paper and the candidate’s subcorpus.*** (Read that twice!) This gives equal weight to each feature, no matter how often the words occur in the texts; otherwise, the top 3 or 4 features would overwhelm everything else.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPc-Srze0bxh"
      },
      "source": [
        "# formula Delta = sum(abs())\n",
        "\n",
        "# new data frame for delta\n",
        "dfDelta = dfZ.copy()\n",
        "col_keys = list(authors_no_T)\n",
        "col_vals = list(authors_no_T)\n",
        "for idx, v in enumerate(col_vals):\n",
        "  col_vals[idx] = \"T_to_\" + col_vals[idx]\n",
        "col_labels = dict(zip(col_keys, col_vals))\n",
        "dfDelta = dfDelta.rename(index=str, columns=col_labels)\n",
        "\n",
        "for v in col_vals:\n",
        "  dfDelta[v] = abs(dfDelta[v] - dfDelta[\"T\"])\n",
        "\n",
        "# add a row for sums and and delta and calculate\n",
        "\n",
        "# \n",
        "dfDelta.loc[\"sum\"] = dfDelta.sum(axis=0)\n",
        "dfDelta.loc[\"delta\"] = dfDelta.loc[\"sum\"]/len(features)\n",
        "\n",
        "# clean up stupid error\n",
        "all_col_labels = list(dfDelta.columns)\n",
        "for col in all_col_labels:\n",
        "  if col not in col_vals:\n",
        "    dfDelta.loc[\"sum\",col] = \"\"\n",
        "    dfDelta.loc[\"delta\", col] = \"\"\n",
        "\n",
        "dfDelta.tail(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lRGN-pryV6m"
      },
      "source": [
        "# report out\n",
        "\n",
        "for col in col_vals:\n",
        "  print(\"Delta for \" + col + \" is: \",dfDelta.loc[\"delta\",col])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY9YyB2tvoVp"
      },
      "source": [
        "This was the conclusion/outpt in PH\n",
        "```\n",
        "Delta score for candidate Hamilton is 1.768470453004334\n",
        "Delta score for candidate Madison is 1.6089724119682816\n",
        "**Delta score for candidate Jay is 1.5345768956569326**\n",
        "Delta score for candidate Disputed is 1.5371768107570636\n",
        "Delta score for candidate Shared is 1.846113566619675\n",
        "```\n",
        "\n",
        "This was Laramée's concluding paragraph:\n",
        "\n",
        "> As expected, Delta identifies John Jay as Federalist 64’s most likely author. It is interesting to note that, according to Delta, Federalist 64 is more similar to the disputed papers than to those known to have been written by Hamilton or by Madison; why that might be, however, is a question for another day.\n",
        "\n",
        "Has our work confirmed this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt8sCDu3IEms"
      },
      "source": [
        "# Stylometry II: Visualizing Similarity and Difference\n",
        "Here what we are doing, is essentially repeating what we did when applying Burrow's Delta, with two important changes: (1) we are applying it to each publication separately; and (2) we are comparing every work to every other work to gauge \"closeness\" or \"distance.\"   \n",
        "As with topic models or word vector embeddings, we can think of each sample as a vector and use a distance measurement to position them in multidimensional space.\n",
        "Here there's a problem: how do we examine or visualize these relationships, and how do we find the important ones? (With 30 features, we need to allow up to 29 dimensions to describe the variation. Humans have difficulty imagining more than three.)   \n",
        "We are going to demo two techniques for visualizing multi-dimensional data: **Principal Component Analysis** (PCA) and **t-distributed stochastic neighbor embedding**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   \n",
        "## PCA\n",
        "**Principal Component Analysis** attempts to calculate the directions of the dimensions and their weights. When we are looking at a two dimensional plot of principal components, we are looking at two specific dimensions calculated by the model. The analysis also can tell us how much of the variation among our test cases is explained by the specific components.   \n",
        "In our case, using the same number of features as before, the first two components account for a little under a quarter of the variation among the documents, but it looks like the first component (the horizontal or x axis) is where a good deal of the author differentiation is happening."
      ],
      "metadata": {
        "id": "FVUBMGG4xl9v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6dSr9dg8t1M"
      },
      "source": [
        "## Prepare the data as for Burrows' Delta\n",
        "Here we go over the steps that we followed for Burrow's delta to get a frequency table for each individual publication in the Federalist Papers.    \n",
        "We then reformat the data in a transposed table for PCA. (Without transposing, PCA would calculate how much features differ from one another over the 85D space of the authors. A very different question.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLloEreA8wno"
      },
      "source": [
        "# Repeat earlier steps, but now asigning each paper not author to a column \n",
        "# copy features and corpus data\n",
        "\n",
        "# repeating here allows us to experiment with feature values\n",
        "# see the explanations above\n",
        "\n",
        "corpus_tokens = corpus.split()\n",
        "\n",
        "# create frequency list using built in nltk function\n",
        "whole_corpus_freq_dist = list(nltk.FreqDist(corpus_tokens).most_common(30))\n",
        "\n",
        "# # uncomment and run again to see the check the first 10\n",
        "# print(whole_corpus_freq_dist[ :10 ])\n",
        "\n",
        "# data structure to contain our statistical information\n",
        "dfFeatures_pca = pd.DataFrame( columns=[\"feats\",\"corpus\"])\n",
        "dfFeatures_pca[\"feats\"] = [w for w, freq in whole_corpus_freq_dist]\n",
        "dfFeatures_pca[\"corpus\"] = [freq for w, freq in whole_corpus_freq_dist]\n",
        "\n",
        "# the last time we did this we bundled the docs together by attribution \n",
        "# This time, we create frequency table by publication rather than author\n",
        "\n",
        "for p in range(1,len(dfPapers)+1):  # iterate over all the files\n",
        "  paper_corpus = ' '.join(dfPapers[\"text\"][dfPapers[\"num\"] == p].values)\n",
        "\n",
        "  #separate into a list of tokens (features)\n",
        "  paper_tokens = paper_corpus.split()\n",
        "  \n",
        "  paper_length = len(paper_tokens)\n",
        "\n",
        "  # copy the features to a list\n",
        "  # for each feature count the proportion of features to total author words\n",
        "  # append to df\n",
        "\n",
        "  # create frequency list using built in nltk function\n",
        "  features = dfFeatures_pca.feats.to_list()\n",
        "  \n",
        "  # # raw numbers\n",
        "  # paper_features = [paper_tokens.count(x) for x in features]\n",
        "\n",
        "  # proportions\n",
        "  paper_features = [paper_tokens.count(x)/paper_length for x in features]\n",
        "  dfFeatures_pca[p] = paper_features\n",
        "\n",
        "\n",
        "# transpose\n",
        "dfFeats_transp = dfFeatures_pca.transpose()\n",
        "dfFeats_transp.drop([\"feats\", \"corpus\"],inplace=True)\n",
        "\n",
        "for f in features:\n",
        "  dfFeats_transp.rename(columns={features.index(f):f},inplace=True)\n",
        "\t\n",
        "\n",
        "# # uncomment to show check data\n",
        "dfFeats_transp.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaVnZPyEcm3p"
      },
      "source": [
        "##Calcuate PCA \n",
        "We are using tools built into the scikit-learn library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsjFcX8Ocnf4"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "# # # to scale (mean = 0 total stdev = 1)\n",
        "# # # balances the weights of more q less frequent words\n",
        "# scaled = StandardScaler().fit_transform(dfFeats_transp)\n",
        "# principalComponents = pca.fit_transform(scaled)\n",
        "\n",
        "# to apply scaling uncomment above and comment next line\n",
        "principalComponents = pca.fit_transform(dfFeats_transp)\n",
        "principalDf = pd.DataFrame(data = principalComponents,\n",
        "                           columns = ['principal component 1', \n",
        "                           'principal component 2', \n",
        "                           'principal component 3'])\n",
        "\n",
        "# add attrib labels\n",
        "principalDf[\"attrib\"] = dfPapers[\"attrib\"]\n",
        "\n",
        "principalDf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXJybJG4g5NG"
      },
      "source": [
        "## Plot a 2-Dimensional Grid for two Components.\n",
        "Plot a 2D scatter chart for the two first principal components.  \n",
        "Federalist 64 is marked in red on the plot. Does this confirm our earlier obsevation using Burrows's Delta?  \n",
        "This was Laramée's concluding paragraph:\n",
        ">As expected, Delta identifies John Jay as Federalist 64’s most likely author. It is interesting to note that, according to Delta, Federalist 64 is more similar to the disputed papers than to those known to have been written by Hamilton or by Madison; why that might be, however, is a question for another day.   \n",
        "\n",
        "Is this still true?  \n",
        "What happens when we increase the number of features?\n",
        "Decrease?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmuHWbJdg8Dp"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "## plotting code from:\n",
        "## https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
        "\n",
        "# print out the amount explained by the first two components\n",
        "print(pca.explained_variance_ratio_)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = [\"H\",\"M\",\"J\",\"S\",\"D\",\"T\"]\n",
        "colors = ['purple', 'green', 'black','yellow', 'blue', 'red']\n",
        "for target, color in zip(targets,colors):\n",
        "  indicesToKeep = principalDf['attrib'] == target\n",
        "  ax.scatter(principalDf.loc[indicesToKeep, 'principal component 1']\n",
        "               , principalDf.loc[indicesToKeep, 'principal component 2']\n",
        "               , c = color\n",
        "               , s = 50)\n",
        "ax.legend(targets)\n",
        "ax.grid()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## t-SNE\n",
        "**t-distributed stochastic neighbor embedding** is another widely used method for visualizing high-dimensional data in 2- to 3-dimensional space. \n",
        "It assigns a probability that each datapoint is similar to every other datapoint, and then attempts to map those relationships in a lower dimensional space. \n",
        "\n",
        "A frequent description is that PCA attempts to preserve the overall structure of the data as a whole, while t-SNE preserves the relationship with neighbors."
      ],
      "metadata": {
        "id": "GEQSsaPt0nbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating t-SNE\n",
        "Although the math of the two approaches is different, in scikitlearn, the procedure is more or less the same. \n",
        "\n",
        "Our data is the same `dFeats_transp` we created for PCA: a table that presents each of the words (features) as columns and each of the documents as rows.\n",
        "\n",
        "* set the parameters for the t-SNE model we are using\n",
        "\n",
        "* fit and transform our data to the model to generate an array of points\n",
        "\n",
        "* plot those points in 2-dimensional space\n"
      ],
      "metadata": {
        "id": "HPyidrrJmW2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/olekscode/Examples-PCA-tSNE/blob/master/Python/Visualizing%20Iris%20Dataset%20using%20PCA%20and%20t-SNE.ipynb\n",
        "\n",
        "# We set the parameters for t-SNE\n",
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(n_components=2, \n",
        "            n_iter=1000, \n",
        "            random_state=0, \n",
        "            perplexity=15, \n",
        "            learning_rate='auto',\n",
        "            init='random')\n",
        "points = tsne.fit_transform(dfFeats_transp[features])"
      ],
      "metadata": {
        "id": "Pk63ZvYKCkAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsneDF = pd.DataFrame(data = points,\n",
        "                           columns = ['eigenvector 1', \n",
        "                           'eigenvector 2'])\n",
        "tsneDF[\"attrib\"] = dfPapers[\"attrib\"]\n",
        "tsneDF"
      ],
      "metadata": {
        "id": "XImziqmrFn9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig_tsne = plt.figure(figsize = (8,8))\n",
        "ax = fig_tsne.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Eigenvector 1', fontsize = 15)\n",
        "ax.set_ylabel('Eigenvector 2', fontsize = 15)\n",
        "ax.set_title('2 component t-SNE', fontsize = 20)\n",
        "targets = [\"H\",\"M\",\"J\",\"S\",\"D\",\"T\"]\n",
        "colors = ['purple', 'green', 'black','yellow', 'blue', 'red']\n",
        "for target, color in zip(targets,colors):\n",
        "  indicesToKeep = tsneDF['attrib'] == target\n",
        "  ax.scatter(tsneDF.loc[indicesToKeep, 'eigenvector 1'], \n",
        "             tsneDF.loc[indicesToKeep, 'eigenvector 2'],\n",
        "             c = color, \n",
        "             s = 50)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "metadata": {
        "id": "lsE21ejWDZM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic extraction: TF-IDF and LDA Topic Modelling"
      ],
      "metadata": {
        "id": "JMSQy3Hlroa6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF\n",
        "In our WordCloud experiment, we have already seen that term-frequency (how often a word appears) may not be the most helpful if we are trying to extract meaning. In that experiment we actively excluded stopwords, really frequent words, in order to get at words that are more indicative of what a text is about. **text frequency-inverse document freqency** (TF-IDF) is a method for weighting the \"important\" words in a given document. "
      ],
      "metadata": {
        "id": "_Eo1all5jBLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The intuition behind TF-IDF is pretty straightforward. In the sample of five titles from the Federalist Papers below, the word 'the' is very common in general and it appears in every title at least once. This means that it is not likely to be a good indicator of the contents of the individual document. However each title also has at least one word (e.g., `'foreign,' 'territory,' 'considered,' 'department,' 'senate'`). Most of these (maybe not 'considered', or 'method' in Federalist 49) point to subject matter. \n",
        "\n",
        "|index|file\\_name|title|\n",
        "|---|---|---|\n",
        "|2|federalist\\_3|The Same Subject Continued \\(Concerning Dangers From Foreign Force and|\n",
        "|13|federalist\\_14|Objections to the Proposed Constitution From Extent of Territory|\n",
        "|41|federalist\\_42|The Powers Conferred by the Constitution Further Considered|\n",
        "|48|federalist\\_49|Method of Guarding Against the Encroachments of Any One Department of|\n",
        "|63|federalist\\_64|The Powers of the Senate|\n",
        "\n",
        "In the title to Federalist 2 the **term frequency** of 'the' and 'foreign' are the same: 1 out of 10 or 0.1\n",
        "\n",
        "**document frequency** refers to the proportion of documents in which the word appears. For 'the' that is 5/5 or 1. For 'foreign' in the title of Federalist 2 the document frequency is 1/5 or 0.2\n",
        "\n",
        "**In principle**, for any given term in  any given document we get the TF-IDF score by dividing  the term frequency by the document frequency (multiplying it by the inverse of the document frequency). For 'territory' in the title of Federalist 2, TF-IDF is  0, since the term frequency of 'territory' in that text is 0:\n",
        "\n",
        ">TF-IDF = 0/0.2 = 0\n",
        "\n",
        "The word  'the' has a TF-IDF of:\n",
        "\n",
        ">TF-IDF = 0.1/1 = 0.1\n",
        "\n",
        "For 'foreign':\n",
        ">TF-IDF = 0.1/0.2 = 0.5\n",
        "\n",
        "\n",
        "By taking into account the document frequency of 'the' and 'foreign' we count 'foreign' as five times as \"important\" for this text as 'the.'\n",
        "\n",
        "Note that **in practice** TF-IDF uses a formula based on the natural logarithm of the document frequency to calculate the IDF."
      ],
      "metadata": {
        "id": "Wn3l-b4RVQ2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# the n most frequent features (words) across the corpus\n",
        "# experiment with different values and see the results in the next cell\n",
        "\n",
        "n = 1000\n",
        "\n",
        "# set up the constraints for the structure that will\n",
        "# represent our text as mathematical vectors\n",
        "tfidf = TfidfVectorizer(\n",
        "    min_df = 5,             # ONLY select words appearing in more than 5 docs\n",
        "    max_df = 0.95,          # DO NOT words appearing in >= 95% of docs\n",
        "    max_features = n,       # the n most frequent words across the corpus\n",
        "    stop_words = 'english'  # remove very common english words\n",
        ")\n",
        "\n",
        "# calculate the tf/idf values for each word in all the documents\n",
        "# sklearn allows us to do both steps in one command\n",
        "\n",
        "# `fit` calculates the df and idf values\n",
        "# and the parameters to create a standardized array\n",
        "tfidf.fit(dfPapers.text)\n",
        "\n",
        "# transform creates a (sparse) array of values for each feature for each doc\n",
        "text = tfidf.transform(dfPapers.text)\n"
      ],
      "metadata": {
        "id": "0yPV4Lr0N7Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell to get the first 100 feature names (words) \n",
        "# based on different values of n\n",
        "tfidf.get_feature_names_out()[:100]"
      ],
      "metadata": {
        "id": "erT_xUgtFQYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfTFIDF = pd.DataFrame(text.todense())\n",
        "dfTFIDF.columns = tfidf.get_feature_names_out()\n",
        "dfTFIDF"
      ],
      "metadata": {
        "id": "AZ7gP6AaBXM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10 # number of top keywords to return\n",
        "dfPapers['tfidf_keywords'] = dfTFIDF.apply(lambda x: \n",
        "                                      ', '.join(x.nlargest(n).index.tolist()), \n",
        "                                      axis=1)\n",
        "dfPapers[['num','title','tfidf_keywords']]"
      ],
      "metadata": {
        "id": "Bi9D_LeFykCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA topic modeling\n",
        "***To do***: rewrite exercise using sklearn."
      ],
      "metadata": {
        "id": "wThaZ07yp0Mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.feature_extraction.text as text\n",
        "\n",
        "# min_df: ignore words occurring in fewer than `n` documents \n",
        "#         (if decimal, proportion of docs)\n",
        "# max_df: ignore words occurring in more than `m` documents\n",
        "#         (if decimal, proporition of docs)\n",
        "# stop_words: ignore very common words (\"the\", \"and\", \"or\", \"to\", ...)\n",
        "vec = text.CountVectorizer(min_df=5, \n",
        "                           max_df=0.5, \n",
        "                           stop_words='english')\n",
        "dtm = vec.fit_transform(dfPapers['text'])\n",
        "\n",
        "print(f'Shape of document-term matrix: {dtm.shape}. '\n",
        "      f'Number of tokens {dtm.sum()}')\n"
      ],
      "metadata": {
        "id": "pmhmJTlDp8pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter estimation: using sklearn's default estimation method in the \n",
        "# class LatentDirichletAllocation: variational inference\n",
        "\n",
        "# THIS CAN TAKE A WHILE\n",
        "\n",
        "import sklearn.decomposition as decomposition\n",
        "\n",
        "# define model\n",
        "model = decomposition.LatentDirichletAllocation(\n",
        "    n_components=10, learning_method='online', random_state=1)\n",
        "\n",
        "# does this iterate over and fit each row of dtm?\n",
        "# mixing weights for each document\n",
        "document_topic_distributions = model.fit_transform(dtm)"
      ],
      "metadata": {
        "id": "HmiI463ACYPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get array of \"columns\" (unique words)\n",
        "vocabulary = vec.get_feature_names_out()"
      ],
      "metadata": {
        "id": "LugbmltjCq7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary"
      ],
      "metadata": {
        "id": "T2YIrMSeOmaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create recognizable topic names\n",
        "# add one to topic names to align with pyLDAvis (below)\n",
        "\n",
        "topic_names = [f'Topic {str(k + 1)}' for k in range(10)]\n",
        "\n",
        "# save topic word distributions and document topic distributions in separate dfs\n",
        "topic_word_distributions = pd.DataFrame(\n",
        "    model.components_, columns=vocabulary, index=topic_names)\n",
        "document_topic_distributions = pd.DataFrame(\n",
        "    document_topic_distributions, columns=topic_names, index=dfPapers.index)\n",
        "\n",
        "document_topic_distributions.loc[9]"
      ],
      "metadata": {
        "id": "IB5o0ue_DAp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sort to find the topics with most weight in specific dissent\n",
        "topic_dist_test = document_topic_distributions.loc[11]\n",
        "topic_dist_test.sort_values(ascending=False)[:5]"
      ],
      "metadata": {
        "id": "4BT-vWPOPfVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# topic_word_distributions"
      ],
      "metadata": {
        "id": "z2KNBv9PH66W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create df with topic names as index and joined top n words as content\n",
        "# function to get the top n words sorted\n",
        "def word_dist_for_topic(topic,n):\n",
        "    \"\"\"\n",
        "    returns the top n words for specified topic\n",
        "    \"\"\"\n",
        "    twd = topic_word_distributions.loc[topic].sort_values(ascending=False).head(n)\n",
        "    return twd\n",
        "    \n",
        "n = 10 # number of words to return\n",
        "\n",
        "# word_dist_for_topic = word_dist_for_topic(topic,n)\n",
        "# topic_top_words_joined = \", \".join(word_dist_for_topic.index)\n",
        "# print(topic, ': ',topic_top_words_joined)\n",
        "\n",
        "topics_dict = dict()\n",
        "for t in topic_names:\n",
        "    topic_words = word_dist_for_topic(t,n)\n",
        "    topic_top_words_joined = \", \".join(topic_words.index)\n",
        "    topics_dict[t]=topic_top_words_joined\n",
        "\n",
        "topics_df = pd.DataFrame(topics_dict.items(),columns=['topic','topic words'])  \n",
        "topics_df.set_index('topic')\n",
        "# pd.set_option('display.max_colwidth', None)\n",
        "# # reset_option('display.max_colwidth') #remember to turn on and off\n",
        "topics_df"
      ],
      "metadata": {
        "id": "yEDCEpfULc1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# topics (and topic words) most closely associated with document \n",
        "df = pd.DataFrame(document_topic_distributions.idxmax(axis=1))\n",
        "df['lda_keywords'] = df[0].apply(lambda x : \n",
        "                           .loc[topics_df['topic'] == x, 'topic words'].item())\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "lNC-ViJ8f7j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfPapers.join(df)[['num','title','tfidf_keywords',0,'lda_keywords']]"
      ],
      "metadata": {
        "id": "56MTBptvPbmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# documents most strongly associated with each topic\n",
        "d = document_topic_distributions.T.eq(document_topic_distributions.T.max(axis=1), axis=0)\n",
        "d.dot(d.columns)"
      ],
      "metadata": {
        "id": "kAkUjhItdV_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Space"
      ],
      "metadata": {
        "id": "57a6TTlzta6K"
      }
    }
  ]
}