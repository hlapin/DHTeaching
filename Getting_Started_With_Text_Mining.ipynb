{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Getting Started With Text Mining",
      "provenance": [],
      "collapsed_sections": [
        "lNhiDmNayrJl",
        "Izw6cSMFqVvW"
      ],
      "authorship_tag": "ABX9TyMj5WwTBgTwdtNUfOA4MBVP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hlapin/DHTeaching/blob/master/Getting_Started_With_Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjUGJpdg-Qi5"
      },
      "source": [
        "# Getting the text\r\n",
        "We are going to be working with the Federalist Papers  \r\n",
        "We need to:\r\n",
        "1. Download a zip file from github (Programming Historian repository)\r\n",
        "2. Unzip it and\r\n",
        "3. Unpack the files in a local folder [local to Colab]\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5VYgHAGFL4C"
      },
      "source": [
        "import requests, io, zipfile, os\r\n",
        "os.chdir ('/content/') # changes working directory on local machine\r\n",
        "r = requests.get('https://github.com/programminghistorian/ph-submissions/blob/gh-pages/assets/introduction-to-stylometry-with-python/stylometry-federalist.zip?raw=true')\r\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\r\n",
        "z.extractall() \r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tc2wcWXMrZc"
      },
      "source": [
        "There is now a local file on the virtual machine you are using called `data` with the files   \r\n",
        "You can check that in the tools in the left hand toolbar   \r\n",
        "However, let's set the working directory to that directory confirm it and list its contents\r\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mF_YzN2xIpe"
      },
      "source": [
        "os.chdir ('/content/data') # changes working directory on local machine\r\n",
        "print ('current working directory:')\r\n",
        "print (os.getcwd()) # prints path to current working directory \r\n",
        "print ('file list:')\r\n",
        "%ls #instruction to list file directory\r\n",
        "\r\n",
        "# did you not get a file list? a directory? if not we have a problem."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0giYZI2WlYjN"
      },
      "source": [
        "## Text to Dataframe\r\n",
        "Now we are going to read the complete work `federalist.txt` into one variable (`papersAll`) and each of the individual chapters into a table (a pandas dataframe, `dfPapers`) where each row is a document and that has the column headings `file_name` and `text`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJWhHkFuRJNR"
      },
      "source": [
        "# \r\n",
        "import pandas as pd # pandas is a data structure library\r\n",
        "import glob\r\n",
        "\r\n",
        "with open (\"federalist.txt\",\"r\") as f:\r\n",
        "  papersAll = f.read().replace('\\n', ' ')\r\n",
        "\r\n",
        "print(papersAll[:100]) #print first 100 chars to check\r\n",
        "\r\n",
        "# create a dataframe with each document as a row\r\n",
        "results = {\"file_name\":[],\"text\":[]}\r\n",
        "for item in glob.glob('*[0-9].txt'):  # read only files with numerals\r\n",
        "   short = item.split('.')[0]\r\n",
        "   with open(item, \"r\") as file_open:\r\n",
        "     results[\"file_name\"].append(short)\r\n",
        "     results[\"text\"].append(file_open.read().replace('\\n', ' '))\r\n",
        "dfPapers = pd.DataFrame(results)\r\n",
        "\r\n",
        "#let's check that we got the documents into shape\r\n",
        "print(dfPapers.head)           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWozCPbBokdF"
      },
      "source": [
        "## A Bit of Cleanup\r\n",
        "Let's remove punctuation and convert all upper case to lower case, and then print a sample of our data to if we got it right.  \r\n",
        "> *Regular expressions* refers to a set of operations on text that can be defined by patterns (a valid email address is an unbroken string, followed by '@' followed by a domain and a valid suffix or suffixes (.org, .edu, .ac.uk).\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yXp9DXZoXk8"
      },
      "source": [
        "import re #re is the module that does regular expression operations\r\n",
        "\r\n",
        "# note that pandas allows us to operate on all the cells in a column\r\n",
        "# of a dataframe by filtering by column label: dfPapers['text'] \r\n",
        "\r\n",
        "# regularize spacing: \r\n",
        "# replace one or more line breaks or spaces with single space\r\n",
        "dfPapers['text'] = dfPapers['text'].map(lambda x: re.sub('\\s+', ' ', x))\r\n",
        "\r\n",
        "# remove sentence punctuation. This time replace with no space\r\n",
        "dfPapers['text'] = dfPapers['text'].map(lambda x: re.sub('[\\(\\(\\:;,\\.!?]', '', x))\r\n",
        "\r\n",
        "# convert characters to lower case\r\n",
        "dfPapers['text'] = dfPapers['text'].map(lambda x: x.lower())\r\n",
        "\r\n",
        "#let's check that we got the documents into shape\r\n",
        "print(dfPapers.head)     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMXhkotPt7Cj"
      },
      "source": [
        "# Some Exploratory Analysis\r\n",
        "First we are going to do some exploratory text analysis by making a word cloud.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-Sljwkkv5Vd"
      },
      "source": [
        "from wordcloud import WordCloud\r\n",
        "\r\n",
        "\r\n",
        "# code adapted from https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\r\n",
        "\r\n",
        "# Join the different processed titles together into one long text.\r\n",
        "long_string = ','.join(list(dfPapers['text'].values))\r\n",
        "\r\n",
        "# Create a WordCloud object\r\n",
        "# You can change the parameters below\r\n",
        "wordcloud = WordCloud(background_color=\"white\", \r\n",
        "                        max_words=1000, \r\n",
        "                        contour_width=3, \r\n",
        "                        contour_color='steelblue')\r\n",
        "\r\n",
        "# Generate a word cloud\r\n",
        "wordcloud.generate(long_string)\r\n",
        "\r\n",
        "# Visualize the word cloud\r\n",
        "wordcloud.to_image()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogC-BFLy5luF"
      },
      "source": [
        "# Topic Modelling\r\n",
        "## Latent Dirichlet Allocation (LDA)\r\n",
        "A document is a collection of topics.\r\n",
        "Topics are lists of words that appear frequently in those topics  \r\n",
        "A recipe has:\r\n",
        "* some words having to do with `food` (milk, eggs, a live badger)\r\n",
        "* some having to do with `operations` (mix, heat, beat, burn)\r\n",
        "* it will also include a bunch of other stuff (esp. on a food blog) - family, geography, calendar; perhaps gender, ethnicity.  \r\n",
        "\r\n",
        "LDA uses a machine learning algorithm to calculate a pre-set number of topics and by looking at what words tend to go together tries to learn the underlying topics.  \r\n",
        "The topic listings are not mutually exclusive; the same word can appear in more than one topic.\r\n",
        "\r\n",
        "## How it works\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXSCZPVX_P0P"
      },
      "source": [
        "re "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNhiDmNayrJl"
      },
      "source": [
        "\r\n",
        "## Topic Modeling Operations\r\n",
        "We are running MALLET, a java program, through the python gensim library. This seems to give getter results than gensim's own lda algorithm.\r\n",
        "We are then going to convert that model into gensim's own model so that we can use visualization tools.  \r\n",
        "1. Import/update various Python libraries we will be using\r\n",
        "2. Import MALLET and deploy it. \r\n",
        "3. Prepare our data for further processing\r\n",
        "1. Create the \"corpus\" and the \"dictionary\" required  for analysis\r\n",
        "2. Create an LDA model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbI-2c_0Al6v"
      },
      "source": [
        "## Import/update libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXTz9Vw09HFb"
      },
      "source": [
        "# much of the following repurposes:\r\n",
        "# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\r\n",
        "\r\n",
        "# Bunch o' modules we will be using\r\n",
        "import numpy as np  # a library for arrays\r\n",
        "from pprint import pprint # formats output (\"pretty-prints\")\r\n",
        "\r\n",
        "# Gensim package for data text analysis\r\n",
        "!pip install --upgrade gensim\r\n",
        "import gensim\r\n",
        "import gensim.corpora as corpora\r\n",
        "from gensim.utils import simple_preprocess\r\n",
        "from gensim.models import CoherenceModel\r\n",
        "\r\n",
        "# Plotting tools\r\n",
        "!pip install pyLDAvis\r\n",
        "import pyLDAvis\r\n",
        "import pyLDAvis.gensim  # don't skip this\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "# we are also going to want  remove \"stopwords\r\n",
        "import nltk\r\n",
        "#from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "# run with the following un-commented to see what the stopwords are\r\n",
        "# these are based on contemporary English; we'd need to do some linguistic work\r\n",
        "# for the Federalist Papers\r\n",
        "\r\n",
        "# print(len(stop_words))\r\n",
        "# print(stop_words)\r\n",
        "\r\n",
        "\r\n",
        "# # Enable logging for gensim - optional\r\n",
        "# import logging\r\n",
        "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrPjuM8kF6CC"
      },
      "source": [
        "## Prepare our data for processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA0JwXj7yTTT"
      },
      "source": [
        "# Get our data in the form of a list in which every FP document is \r\n",
        "# Represented as a list of words.\r\n",
        "\r\n",
        "data = dfPapers['text'].values.tolist()\r\n",
        "# uncommented below will print the text of the first text\r\n",
        "# pprint(data[:1])\r\n",
        "\r\n",
        "#now remove stopwords\r\n",
        "def paper_to_words(papers):\r\n",
        "  \"\"\" Our first function! Yay!\r\n",
        "      this function does the tokenization\"\"\"\r\n",
        "  for paper in papers:\r\n",
        "    yield(gensim.utils.simple_preprocess(str(paper), deacc=True))\r\n",
        "\r\n",
        "def remove_stopwords(texts):\r\n",
        "  \"\"\" This function checks for and removes stopwords\"\"\"\r\n",
        "  return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\r\n",
        "\r\n",
        "      \r\n",
        "data_words = list(paper_to_words(data))\r\n",
        "data_words_no_stops = remove_stopwords(data_words)\r\n",
        "\r\n",
        "# run with the following un-commented to see the first document\r\n",
        "# print(data_words_no_stops[:1])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fod54TFGHTH"
      },
      "source": [
        "## Prepare the \"corpus\" and \"dictionary\" required as input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBDcjxQqGUiJ"
      },
      "source": [
        "# Create Dictionary\r\n",
        "id2word = corpora.Dictionary(data_words_no_stops)\r\n",
        "\r\n",
        "# Create Corpus\r\n",
        "texts = data_words_no_stops\r\n",
        "\r\n",
        "# Term Document Frequency: converts orderd string to \"bag of words\"\r\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\r\n",
        "\r\n",
        "# # viewing data\r\n",
        "# # uncomment and run to view\r\n",
        "# corpus\r\n",
        "# print(corpus[:1])\r\n",
        "\r\n",
        "# # Human readable format of corpus (term-frequency)\r\n",
        "# print(len(id2word))\r\n",
        "# [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VfWTLUdBx4G"
      },
      "source": [
        "## Get, deploy MALLET; if necessary get Java."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wIHXzlAMtTN"
      },
      "source": [
        "# # un-comment if you need to update java\r\n",
        "# def install_java():\r\n",
        "#   !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\r\n",
        "#   os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\r\n",
        "#   !java -version       #check java version\r\n",
        "# install_java()\r\n",
        "\r\n",
        "# getting Mallet\r\n",
        "r = requests.get('http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip')\r\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\r\n",
        "z.extractall('/content') \r\n",
        "\r\n",
        "!chmod 764 /content/mallet-2.0.8/bin/mallet #gives owner (you) execute rights (7)\r\n",
        "\r\n",
        "# just in case we need to set the environmental variables.\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ['MALLET_HOME'] = '/content/data/mallet-2.0.8/'\r\n",
        "mallet_path = '/content/mallet-2.0.8/bin/mallet' \r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6biEIg18ILnT"
      },
      "source": [
        "## Build and view the LDA model in Mallet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofEJNFxoKVX6"
      },
      "source": [
        "\r\n",
        "from gensim.models.wrappers import LdaMallet\r\n",
        "\r\n",
        "# can experiment with the number of topics, and the optimize interval\r\n",
        "num_topics = 50 # 20 is the default\r\n",
        "optimize_interval = 10 ## allows mallet to make some topics more prominent than others\r\n",
        "                       ## MALLET specs say \"10 is reasonable\"\r\n",
        "                       ## reportedly 20-40\r\n",
        "                       ## set to zero to turn off\r\n",
        "\r\n",
        "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, \r\n",
        "                                             corpus=corpus, \r\n",
        "                                             num_topics=num_topics, \r\n",
        "                                             optimize_interval=optimize_interval,\r\n",
        "                                             id2word=id2word)\r\n",
        "\r\n",
        "# Show Topics and coherence score\r\n",
        "pprint(ldamallet.show_topics(formatted=False))\r\n",
        "coherence_model_ldamallet = CoherenceModel(model=ldamallet, \r\n",
        "                                           texts=data_words_no_stops, \r\n",
        "                                           dictionary=id2word, coherence='c_v')\r\n",
        "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\r\n",
        "print('\\nCoherence Score: ', coherence_ldamallet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG6meyc1uAT-"
      },
      "source": [
        "## Let's do some basic examination\r\n",
        "\r\n",
        "The next block builds a table that gives the document mostly closely associated with a topic\r\n",
        "> To do: \r\n",
        "* Text operations on original data structure to make sure that sort order is the same as alpha/numerical\r\n",
        "* Extract publication name (Federalist no. ##) to use in table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGnG8yAF6eCH"
      },
      "source": [
        "\r\n",
        "%load_ext google.colab.data_table\r\n",
        "\r\n",
        "def format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=data_words_no_stops):\r\n",
        "    # Init output\r\n",
        "    paper_topics_df = pd.DataFrame()\r\n",
        "\r\n",
        "    # Get main topic in each document\r\n",
        "    for i, row in enumerate(ldamodel[corpus]):\r\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\r\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\r\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\r\n",
        "            if j == 0:  # => dominant topic\r\n",
        "                wp = ldamodel.show_topic(topic_num)\r\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\r\n",
        "                paper_topics_df = paper_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\r\n",
        "            else:\r\n",
        "                break\r\n",
        "    paper_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\r\n",
        "\r\n",
        "    # to align MALLET topic indexing [1-based]  with gensim/Python [0-based]\r\n",
        "    paper_topics_df['Dominant_Topic'] = paper_topics_df['Dominant_Topic'] + 1\r\n",
        "\r\n",
        "    # Add original text to the end of the output\r\n",
        "    #contents = pd.Series(texts)\r\n",
        "    # paper_topics_df = pd.concat([paper_topics_df, contents], axis=1)\r\n",
        "    return(paper_topics_df)\r\n",
        "\r\n",
        "\r\n",
        "df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=data_words_no_stops)\r\n",
        "\r\n",
        "# Format\r\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\r\n",
        "\r\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords']\r\n",
        "\r\n",
        "# Show\r\n",
        "df_dominant_topic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izw6cSMFqVvW"
      },
      "source": [
        "## Reformat and view in pyLDAvis\r\n",
        "\r\n",
        "Repackage our MALLET model as a gensim model so we can use the tools  \r\n",
        "\r\n",
        "Lambda metric: proportion of frequency in overall model to \"frequency\"  \r\n",
        "**0 [Red]:** Sorts only by the frequency of the word in the overall topic model  \r\n",
        "**1 [Blue]:** Sorts only by the impact the word on the selected model\r\n",
        "**lambda:** \r\n",
        "> \"the “the ratio of a term’s probability within a topic to its marginal probability across the corpus,” or the ratio between its red bar and blue bar\"  \r\n",
        "[https://we1s.ucsb.edu/research/we1s-tools-and-software/topic-model-observatory/tmo-guide/tmo-guide-pyldavis/]\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWfu7tv2l9pt"
      },
      "source": [
        "# makes use of https://medium.com/@jobethmuncy/formatting-pyldavis-and-mallet-model-7c0d00061b67\r\n",
        "\r\n",
        "# convert to gensim LDA model\r\n",
        "mallet_lda_model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)\r\n",
        "\r\n",
        "pyLDAvis.enable_notebook()\r\n",
        "vis = pyLDAvis.gensim.prepare(mallet_lda_model, corpus, id2word,sort_topics = False)\r\n",
        "pyLDAvis.display(vis)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}