{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Getting Started With Text Mining",
      "provenance": [],
      "collapsed_sections": [
        "aBagCN6tmFeW",
        "ZjUGJpdg-Qi5",
        "0giYZI2WlYjN",
        "hWozCPbBokdF",
        "yMXhkotPt7Cj",
        "ogC-BFLy5luF",
        "lNhiDmNayrJl",
        "ZbI-2c_0Al6v",
        "KrPjuM8kF6CC",
        "7fod54TFGHTH",
        "8VfWTLUdBx4G",
        "CG6meyc1uAT-",
        "kH3519LhH60C",
        "bHvhW9mBIaOY"
      ],
      "authorship_tag": "ABX9TyNoHrLnt3ca46/quF7ir8p1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hlapin/DHTeaching/blob/master/Getting_Started_With_Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBagCN6tmFeW"
      },
      "source": [
        "# Introduction\n",
        "This colab notebook was prepared for teaching purposes for a session on text mining in a course on digital tools in historical research.  \n",
        "As a text source the examples use the text of the *Federalist Papers*.   \n",
        "Much of what follows is derivative, although I have revised material for display or pedagogical purposes. (The goal has been to provide \"plug-and-play\" demonstrations, leaving space open for discussion of methods and concepts.)     \n",
        "I have used the following resources extensively to develop the notebook. (Others are cited as they come up.)  \n",
        "* https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
        "* https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
        "* https://medium.com/@jobethmuncy/formatting-pyldavis-and-mallet-model-7c0d00061b67  \n",
        "* https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjUGJpdg-Qi5"
      },
      "source": [
        "# Getting the text\n",
        "We are going to be working with the Federalist Papers  \n",
        "We need to:\n",
        "1. Download a zip file from github (Programming Historian repository)\n",
        "2. Unzip it and\n",
        "3. Unpack the files in a local folder [local to Colab]\n",
        "\n",
        "After this you ***can*** skip to the Stylometry demo instead of the the LDA topic modelling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5VYgHAGFL4C"
      },
      "source": [
        "import requests, io, zipfile, os\n",
        "os.chdir ('/content/') # changes working directory on local machine\n",
        "r = requests.get('https://github.com/programminghistorian/ph-submissions/blob/gh-pages/assets/introduction-to-stylometry-with-python/stylometry-federalist.zip?raw=true')\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall() \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tc2wcWXMrZc"
      },
      "source": [
        "There is now a local file on the virtual machine you are using called `data` with the files   \n",
        "You can check that in the files tools in the left hand panel.   \n",
        "However, let's set the working directory to that directory confirm it and list its contents programmatically.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mF_YzN2xIpe"
      },
      "source": [
        "os.chdir ('/content/data') # changes working directory on local machine\n",
        "print ('current working directory:')\n",
        "print (os.getcwd()) # prints path to current working directory \n",
        "print ('file list:')\n",
        "%ls #instruction to list file directory\n",
        "\n",
        "# did you not get a file list? a directory? if not we have a problem."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0giYZI2WlYjN"
      },
      "source": [
        "## Text to Dataframe\n",
        "Now we are going to read each of the individual chapters into a table (a pandas dataframe, `dfPapers`) where each row is a document and that has the column headings `file_name` and `text`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJWhHkFuRJNR"
      },
      "source": [
        "# \n",
        "import pandas as pd # pandas is a data structure library\n",
        "import glob\n",
        "\n",
        "# print(papersAll[:100]) #print first 100 chars to check\n",
        "\n",
        "# create a dataframe with each document as a row\n",
        "results = {\"file_name\":[],\"num\":[],\"text\":[]}\n",
        "for item in glob.glob('*[0-9].txt'):  # read only files with numerals\n",
        "   short = item.split('.')[0]\n",
        "   paperNum = int(short.split('_')[1])\n",
        "   with open(item, \"r\") as file_open:\n",
        "     results[\"file_name\"].append(short)\n",
        "     results[\"num\"].append(paperNum)\n",
        "     results[\"text\"].append(file_open.read().replace('\\n', ' '))\n",
        "dfPapers = pd.DataFrame(results)\n",
        "dfPapers = dfPapers.sort_values([\"num\"])\n",
        "dfPapers = dfPapers.reset_index(drop=True)\n",
        "\n",
        "#let's check that we got the documents into shape\n",
        "dfPapers.head (5)           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWozCPbBokdF"
      },
      "source": [
        "## A Bit of Cleanup\n",
        "Let's remove punctuation and convert all upper case to lower case, and then print a sample of our data to if we got it right.  \n",
        "> *Regular expressions* (often `regex`) refers to a set of operations on text that can be defined by patterns (a valid email address is an unbroken string, followed by '@' followed by a domain and a valid suffix or suffixes (.org, .edu, .ac.uk). For an example of how complex the regex may be to capture \"all\" emails see: http://emailregex.com/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yXp9DXZoXk8"
      },
      "source": [
        "import re #re is the module that does regular expression operations\n",
        "\n",
        "# note that pandas allows us to operate on all the cells in a column\n",
        "# of a dataframe by filtering by column label: dfPapers['text'] \n",
        "\n",
        "# regularize spacing: \n",
        "# replace one or more line breaks or spaces with single space\n",
        "dfPapers['text'] = dfPapers['text'].map(lambda x: re.sub(r\"\\s+\", ' ', x))\n",
        "\n",
        "# remove sentence punctuation, numerals, etc. This time replace with no space\n",
        "dfPapers['text'] = dfPapers['text'].map(lambda x: re.sub(r\"[\\d\\'\\\"\\(\\)\\:;,\\.!?‘’“”]\", '', x))\n",
        "\n",
        "# convert characters to lower case\n",
        "dfPapers['text'] = dfPapers['text'].map(lambda x: x.lower())\n",
        "\n",
        "#let's check that we got the documents into proper shape\n",
        "dfPapers.head(5) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMXhkotPt7Cj"
      },
      "source": [
        "# Some Exploratory Analysis\n",
        "First we are going to do some exploratory text analysis by making a word cloud.  \n",
        "How does the model select words to present?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd4b_a82y0xK"
      },
      "source": [
        "## Frequency vs significance\n",
        "Frequency determines the size of the word, but:\n",
        "* Is it really modelling the ***most frequent*** words?\n",
        "* What is the problem with words like may, will?\n",
        "\n",
        "The python function that does this has a default set of stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-Sljwkkv5Vd"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "\n",
        "# code adapted from https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
        "\n",
        "# Join the different processed titles together into one long text.\n",
        "long_string = ','.join(list(dfPapers['text'].values))\n",
        "\n",
        "# Create a WordCloud object\n",
        "# You can change the parameters below\n",
        "wordcloud = WordCloud(background_color=\"white\", \n",
        "                        max_words=100, \n",
        "                        contour_width=3, \n",
        "                        contour_color='steelblue')\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chaZdyi_mPKt"
      },
      "source": [
        "## Let's run the same function without any stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGvLTzWjmORs"
      },
      "source": [
        "stopwords = set() # define the list of stopwords as an empty set \n",
        "\n",
        "# Join the different processed titles together into one long text.\n",
        "long_string = ','.join(list(dfPapers['text'].values))\n",
        "\n",
        "# Create a WordCloud object\n",
        "# You can change the parameters below\n",
        "wordcloud = WordCloud(background_color=\"white\", \n",
        "                        max_words=100, \n",
        "                        contour_width=3, \n",
        "                        stopwords = stopwords,\n",
        "                        collocations=False, #only single tokens\n",
        "                        contour_color='steelblue')\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7Njag8YXdaw"
      },
      "source": [
        "# Topic Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogC-BFLy5luF"
      },
      "source": [
        "## Latent Dirichlet Allocation (LDA)\n",
        "A document is a collection of topics.\n",
        "Topics are lists of words that appear frequently in those topics  \n",
        "A recipe has:\n",
        "* some words having to do with `food` (milk, eggs, a live badger)\n",
        "* some having to do with `operations` (mix, heat, beat, burn)\n",
        "* it will also include a bunch of other stuff -- family, geography, calendar; perhaps gender, ethnicity.  \n",
        "\n",
        "LDA uses a machine learning algorithm to calculate a pre-set number of topics. By looking at what words tend to go together and their frequency the algorithm tries to learn the underlying topics.  \n",
        "The topic listings are not mutually exclusive; the same word can appear in more than one topic.\n",
        "\n",
        "## How it works\n",
        "\n",
        "The principle between LDA is that by iterating through a corpus many times a computer can learn to predict what topic a word belongs to and what topics are at work in any given document.   \n",
        "***Latent*** because we do not have access to the topics before we start. Instead we are making inferences about hidden topics and how they work in the documents we are examining.  \n",
        "***Dirichlet*** because of the initial assumptions about the distribution of words and topics as the \"prior probability\" to be recalculated after an event, in this case the appearance of a word in a document or topic. (The probability model is Bayesian.)  \n",
        "* each time I want to evaluate the importance of a word for a particular topic I have a better sense of how likely it is to appear with the other words in the topic \n",
        "* each time I look through a document I can better represent the probability of a particular topic affecting the the document.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNhiDmNayrJl"
      },
      "source": [
        "\n",
        "## Topic Modeling Operations\n",
        "We are running MALLET, a java program, through the python gensim library. This seems to give getter results than gensim's own lda algorithm.\n",
        "We are then going to convert that model into the form that gensim packages its own models so that we can use visualization tools for gensim.  \n",
        "1. Import/update various Python libraries we will be using\n",
        "2. Import MALLET and deploy it. \n",
        "3. Prepare our data for further processing.   \n",
        ">[To include/remove stopwords there are lines of code that can be commented and/or uncommented] \n",
        "1. Create the \"corpus\" and the \"dictionary\" used  for analysis\n",
        "2. Create an LDA model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbI-2c_0Al6v"
      },
      "source": [
        "## Import/update libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXTz9Vw09HFb"
      },
      "source": [
        "# much of the following repurposes:\n",
        "# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
        "\n",
        "# Bunch o' modules we will be using\n",
        "import numpy as np  # a library for arrays\n",
        "from pprint import pprint # formats output (\"pretty-prints\")\n",
        "\n",
        "# Gensim package for data text analysis\n",
        "!pip install gensim==3.8.3 # during development an update dropped support for Mallet\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Plotting tools\n",
        "!pip install pyLDAvis==3.2.2 # During development an update to 3.3.0 caused errors\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim  # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# we are may want to  remove \"stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# run with the following un-commented to see what the stopwords are\n",
        "# these are based on contemporary English; we'd need to do some linguistic work\n",
        "# for the Federalist Papers\n",
        "\n",
        "# print(len(stop_words))\n",
        "# print(stop_words)\n",
        "\n",
        "\n",
        "# # Enable logging for gensim - optional\n",
        "# import logging\n",
        "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrPjuM8kF6CC"
      },
      "source": [
        "## Prepare our data for processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA0JwXj7yTTT"
      },
      "source": [
        "# Get our data in the form of a list in which every FP document is \n",
        "# Represented as a list of words.\n",
        "\n",
        "data = dfPapers['text'].values.tolist()\n",
        "# uncommented below will print the text of the first text\n",
        "# pprint(data[:1])\n",
        "\n",
        "# tokenize\n",
        "def paper_to_words(papers):\n",
        "  \"\"\" Our first function! Yay!\n",
        "      this function does the tokenization\"\"\"\n",
        "  for paper in papers:\n",
        "    yield(gensim.utils.simple_preprocess(str(paper), deacc=True))\n",
        "\n",
        "#now remove stopwords\n",
        "def remove_stopwords(texts):\n",
        "  \"\"\" This function checks for and removes stopwords\"\"\"\n",
        "  return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "      \n",
        "data_words = list(paper_to_words(data))\n",
        "\n",
        "# # To keep stopwords comment next uncomment following\n",
        "# data_to_use = data_words\n",
        "\n",
        "# To remove stopwords uncomment this line, comment preceding line\n",
        "data_words_no_stops = remove_stopwords(data_words)\n",
        "data_to_use = data_words_no_stops\n",
        "\n",
        "# run with the following un-commented to see the first document\n",
        "# print(data_to_use[:1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fod54TFGHTH"
      },
      "source": [
        "## Prepare the \"corpus\" and \"dictionary\" required as input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBDcjxQqGUiJ"
      },
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_to_use)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_to_use\n",
        "\n",
        "# Term Document Frequency: converts orderd string to \"bag of words\"\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# # viewing data\n",
        "# # uncomment and run to view\n",
        "# corpus\n",
        "# print(corpus[:1])\n",
        "\n",
        "# # Dislay of term frequency that replaces the numerical ID with the word\n",
        "# print(len(id2word))\n",
        "# [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VfWTLUdBx4G"
      },
      "source": [
        "## Get, deploy MALLET; if necessary get Java."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wIHXzlAMtTN"
      },
      "source": [
        "# # un-comment if you need to update java\n",
        "# def install_java():\n",
        "#   !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "#   os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "#   !java -version       #check java version\n",
        "# install_java()\n",
        "\n",
        "# getting Mallet\n",
        "r = requests.get('http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip')\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall('/content') \n",
        "\n",
        "!chmod 764 /content/mallet-2.0.8/bin/mallet #gives owner (you) execute rights (7)\n",
        "\n",
        "# just in case we need to set the environmental variables.\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ['MALLET_HOME'] = '/content/data/mallet-2.0.8/'\n",
        "mallet_path = '/content/mallet-2.0.8/bin/mallet' \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6biEIg18ILnT"
      },
      "source": [
        "## Build and view the LDA model in Mallet\n",
        "\n",
        "`c_v` coherence score is a measure of how frequently words in a topic occur together.    \n",
        "\n",
        "***Note***: The newly issued version of gensim no longer supports the Mallet wrapper. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofEJNFxoKVX6"
      },
      "source": [
        "\n",
        "from gensim.models.wrappers import LdaMallet\n",
        "\n",
        "# can experiment with the number of topics, and the optimize interval\n",
        "num_topics = 30        ## 20 is the default\n",
        "optimize_interval = 30 ## allows mallet to make some topics more prominent than others\n",
        "                       ## MALLET specs say \"10 is reasonable\"\n",
        "                       ## reportedly 20-40 give good results\n",
        "                       ## set to zero to turn off\n",
        "\n",
        "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, \n",
        "                                             corpus=corpus, \n",
        "                                             num_topics=num_topics, \n",
        "                                             optimize_interval=optimize_interval,\n",
        "                                             id2word=id2word)\n",
        "\n",
        "# Show Topics and coherence score\n",
        "pprint(ldamallet.show_topics(formatted=True, num_topics=num_topics))\n",
        "# to do: prettier output of model\n",
        "\n",
        "coherence_model_ldamallet = CoherenceModel(model=ldamallet, \n",
        "                                           texts=data_to_use, \n",
        "                                           dictionary=id2word, coherence='c_v')\n",
        "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_ldamallet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG6meyc1uAT-"
      },
      "source": [
        "## Let's do some basic examination\n",
        "\n",
        "The next block builds a table that gives the document mostly closely associated with a topic.  \n",
        "Note that our document length is long (compared to tweets or book or restaurant reviews) and that all the documents deal with some topics quite heavily (*The **Federalist** Papers*!) but experimenting LDA does give us some sense of how a computer can infer significant clusters of words.   \n",
        "Our settings allowed Mallet to create topics that were weighted differently from one another.   \n",
        "What downside(s) to that?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGnG8yAF6eCH"
      },
      "source": [
        "\n",
        "%load_ext google.colab.data_table\n",
        "\n",
        "def format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=data_to_use):\n",
        "    # Init output\n",
        "    paper_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row in enumerate(ldamodel[corpus]):\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                paper_topics_df = paper_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    paper_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # to align MALLET topic indexing [1-based]  with gensim/Python [0-based]\n",
        "    paper_topics_df['Dominant_Topic'] = paper_topics_df['Dominant_Topic'] + 1\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    # contents = pd.Series(texts)\n",
        "    # paper_topics_df = pd.concat([paper_topics_df, contents], axis=1)\n",
        "    return(paper_topics_df)\n",
        "\n",
        "\n",
        "df_topic_pages_keywords = format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=data_to_use)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic = df_topic_pages_keywords.reset_index()\n",
        "\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords']\n",
        "\n",
        "# increase document number to align with original chapter nos.\n",
        "df_dominant_topic[\"Document_No\"] = df_dominant_topic[\"Document_No\"] + 1\n",
        "df_dominant_topic[\"file_name\"] = dfPapers[\"file_name\"]\n",
        "\n",
        "# reorder ## better way of doing this?\n",
        "column_names = ['Document_No','file_name', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords']\n",
        "df_dominant_topic = df_dominant_topic.reindex(columns=column_names)\n",
        "\n",
        "# Show\n",
        "df_dominant_topic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izw6cSMFqVvW"
      },
      "source": [
        "## Reformat and view in pyLDAvis\n",
        "\n",
        "Repackage our MALLET model as a gensim model so we can use the `pyLDAvis` tools.  \n",
        "\n",
        "**Red** Frequency in particular topic [\"lift\"]  \n",
        "**Blue** Frequency in the model overall [\"frequency\"]  \n",
        "\n",
        "**Lambda metric:** proportion of frequency in overall model to frequency in topic.  \n",
        "> \"the “the ratio of a term’s probability within a topic to its marginal probability across the corpus,” or the ratio between its red bar and blue bar\"  \n",
        "[https://we1s.ucsb.edu/research/we1s-tools-and-software/topic-model-observatory/tmo-guide/tmo-guide-pyldavis/]   \n",
        "\n",
        "When **`lambda`** is set to 1 (default) words are sorted by their frequency in the topic (the \"lift\"). When set to 0 words whose frequency in the topic is similar to to their frequency overall appear at the top.   \n",
        "[Note: Issues with interpretability, display?]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWfu7tv2l9pt"
      },
      "source": [
        "# makes use of https://medium.com/@jobethmuncy/formatting-pyldavis-and-mallet-model-7c0d00061b67\n",
        "\n",
        "# convert to gensim LDA model\n",
        "mallet_lda_model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(mallet_lda_model, corpus, id2word,sort_topics = False)\n",
        "pyLDAvis.display(vis)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQoinGp50cut"
      },
      "source": [
        "# Stylometry\n",
        "Where topic modeling had to sort the most significant words from the most frequent, because semantics were essential, for stylometrics we want patterns of typical practice for authors.   \n",
        "The approach we are taking to stylometry is based on most frequent (\"function\") words. We could refine this further but the basic observation is that no two authors use words (in English) like \"the\" or \"and\" or \"of\" in quite the same way (or punctuate quite the same way). From there techniques can become quite complicated but the basic idea is that is that if we take a number of features (in our case, most frequent words) and see how these are used by each author, we can use this to measure distance between sample texts or authors' corpora. Before we do that we want to standardize our measurements (so that a larger corpus does not outweigh a smaller one) and decide on how to weight the features. (If the word \"the\" is about 10% of all the words it is clearly potentially distinctive; but how much should it count against the next most frequent words?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAC6Puo1X7a7"
      },
      "source": [
        "# Stylometry I Author Identification\n",
        "> Adapted from \"Programming Historian\"   \n",
        "https://doi.org/10.46430/phen0078   \n",
        "\n",
        "This section uses the same texts as before (the Federalist Papers) to illustrate the use of **Burrows's Delta**. What this tries to do is to measure how each writer in a corpus uses \"function words\" (in English, the very common words like \"the\" and \"is\" and \"to\").       \n",
        "`Delta` seeks to aggregate the observations about each of the features we are testing for (we have used the 30 most common words in the document set).  \n",
        "For each of the features we compare the frequency of the word in each of the texts we are examining in comparison with that of the corpus as a whole, and standardize the measurements across the corpus (so that more prolific authors [Hamilton] or more common words do not outweigh all the other authors or features).   \n",
        "  We also hold out Federalist 64 as a test case, and calculate `Delta` between that essay, and the other authors. A smaller `Delta` between texts means that two texts are \"closer\" to each other. \n",
        "\n",
        "***Note***: If you have not started at \"getting the text\" go back to the first section above.   \n",
        "Running those code blocks will return a text in all lower case with punctuation removed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-5h0QRzkYbG"
      },
      "source": [
        "## First let's modify the dataframe we created to add attribution\n",
        "Follows the canonical division plus test case as in PH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-VohefvPrSf"
      },
      "source": [
        "import nltk\n",
        "\n",
        "%unload_ext google.colab.data_table\n",
        "# the \"canonical\" division into authors plus one test case, as in PH\n",
        "papers = {\n",
        "    'Madison': [10, 14, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48],\n",
        "    'Hamilton': [1, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 21, 22, 23, 24,\n",
        "                 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 59, 60,\n",
        "                 61, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,\n",
        "                 78, 79, 80, 81, 82, 83, 84, 85],\n",
        "    'Jay': [2, 3, 4, 5],\n",
        "    'Shared': [18, 19, 20],\n",
        "    'Disputed': [49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 62, 63],\n",
        "    'TestCase': [64]\n",
        "}\n",
        "k, v = list(papers.keys()), list(papers.values())\n",
        "def return_attrib (num):\n",
        "  \"\"\" checks for the document no in lists of values\n",
        "      returns first letter of attribution. \"\"\"\n",
        "  for i in v:\n",
        "    if num in i: \n",
        "      return k[v.index(i)][0]\n",
        "\n",
        "# insert attribution to datatable in first position if it does not exist\n",
        "if not \"attrib\" in dfPapers.columns:\n",
        "  dfPapers.insert(loc=0, column='attrib',value='')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dfPapers[\"attrib\"] = dfPapers[\"num\"].apply(return_attrib)\n",
        "dfPapers.head(5)    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yww1zvE3krXZ"
      },
      "source": [
        "## Feature Selection\n",
        "We are choosing 30 as in PH example.\n",
        "\n",
        "Create a composite feature set for all texts (except test case)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxNB8eixnBJ7"
      },
      "source": [
        "# filter dataframe to exclude testcase\n",
        "# this is a variation of the operation we used above for the word cloud\n",
        "# create a single string\n",
        "corpus = ' '.join(dfPapers[\"text\"][dfPapers[\"attrib\"] != \"T\"].values)\n",
        "\n",
        "#separate into a list of values\n",
        "corpus_tokens = corpus.split()\n",
        "\n",
        "# create frequency list using built in nltk function\n",
        "whole_corpus_freq_dist = list(nltk.FreqDist(corpus_tokens).most_common(30))\n",
        "\n",
        "# # uncomment to see the check the first 10\n",
        "# whole_corpus_freq_dist[ :10 ]\n",
        "\n",
        "# data structure to contain our statistical information\n",
        "dfFeatures = pd.DataFrame( columns=[\"feats\"])\n",
        "dfFeatures[\"feats\"] = [w for w, freq in whole_corpus_freq_dist]\n",
        "dfFeatures[\"corpus\"] = [freq for w, freq in whole_corpus_freq_dist]\n",
        "\n",
        "# calculate frequency for each of the \"authors\"\n",
        "# authors to test\n",
        "authors = (\"H\",\"M\",\"J\",\"S\",\"D\",\"T\")\n",
        "for author in authors:\n",
        "  author_corpus = ' '.join(dfPapers[\"text\"][dfPapers[\"attrib\"] == author].values)\n",
        "\n",
        "  #separate into a list of values\n",
        "  author_tokens = author_corpus.split()\n",
        "\n",
        "  # create frequency list using built in nltk function\n",
        "  author_length = len(author_tokens)\n",
        "\n",
        "  # copy the features to a list\n",
        "  # for each feature count the proportion of features to total author words\n",
        "  # append to df\n",
        "\n",
        "  features = dfFeatures.feats.to_list()\n",
        "  author_features = [author_tokens.count(x)/author_length for x in features]\n",
        "  # dfFeatures[author] = author_features\n",
        "  dfFeatures[author] = author_features\n",
        "\n",
        "dfFeatures.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH3519LhH60C"
      },
      "source": [
        "## Means, Standard Deviation, z-scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am-Ccdc1IbFg"
      },
      "source": [
        "import math\n",
        "# mean of the mean frequency of each feature\n",
        "# exclude testcase from means\n",
        "authors_no_T = [\"H\",\"M\", \"J\", \"S\", \"D\"]\n",
        "\n",
        "#calculate the means of columns\n",
        "dfFeatures[\"means\"] = dfFeatures[authors_no_T].mean(axis=1)\n",
        "\n",
        "# dfFeatures\n",
        "\n",
        "# calculate stdev of columns as for sample\n",
        "# formula stdev = sum(sqrt((x[i] - x[sample])^2/(n - 1)))\n",
        "# should be a more efficient way of doing this in Pandas but\n",
        "# (a) I am a newbie\n",
        "# (b) this makes the process explicit\n",
        "\n",
        "n = len(authors_no_T)\n",
        "stdev = list([0]*len(features))\n",
        "\n",
        "for i in range(len(features)):\n",
        "  squ_diff_fr_mean = 0\n",
        "  sum_squ_diff = 0\n",
        "  author_feature_values = dfFeatures.iloc[[i],[2,3,4,5,6]].values[0]\n",
        "  feature_mean = dfFeatures.iloc[[i],[8]].values[0]\n",
        "  \n",
        "  for j in range(len(authors_no_T)):\n",
        "    squ_diff_fr_mean = (author_feature_values[j] - feature_mean[0])**2\n",
        "    sum_squ_diff = sum_squ_diff + squ_diff_fr_mean\n",
        "    stdev[i] = math.sqrt(sum_squ_diff/(n - 1))\n",
        "\n",
        "\n",
        "dfFeatures[\"stdev\"] = stdev\n",
        "\n",
        "# z-scores\n",
        "# formula z[j] = (Observed[j] - mean[j])/stdev[j]\n",
        "\n",
        "# dataframe to hold z scores\n",
        "z_cols = list(authors)\n",
        "z_cols.extend([\"means\", \"stdev\"])\n",
        "#z_cols\n",
        "#calcuate z-scores\n",
        "dfZ = dfFeatures[z_cols].copy()\n",
        "for author in authors:\n",
        "  dfZ[author] = (dfZ[author] - dfZ[\"means\"])/dfZ[\"stdev\"]\n",
        "dfZ.head(7)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmVv74MRxC4Z"
      },
      "source": [
        "Sample values in PH for T (Federalist 64) are:\n",
        "```\n",
        "Test case z-score for feature the is -0.7692828380408238\n",
        "Test case z-score for feature of is -1.8167784558461264\n",
        "Test case z-score for feature to is 1.032705844508835\n",
        "Test case z-score for feature and is 1.0268752924746058\n",
        "Test case z-score for feature in is 0.6085448501260903\n",
        "Test case z-score for feature a is -0.9341289591084886\n",
        "Test case z-score for feature be is 1.0279650702511498\n",
        "```\n",
        "Our values are close to those calculated there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHvhW9mBIaOY"
      },
      "source": [
        "## Calculate Burrows's Delta  \n",
        "> (from PH)  \n",
        "Finally, calculate a delta score comparing the anonymous paper with each candidate’s subcorpus. To do this, take the average of the ***absolute values of the differences between the z-scores for each feature between the anonymous paper and the candidate’s subcorpus.*** (Read that twice!) This gives equal weight to each feature, no matter how often the words occur in the texts; otherwise, the top 3 or 4 features would overwhelm everything else.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPc-Srze0bxh"
      },
      "source": [
        "# formula Delta = sum(abs())\n",
        "\n",
        "# new data frame for delta\n",
        "dfDelta = dfZ.copy()\n",
        "col_keys = list(authors_no_T)\n",
        "col_vals = list(authors_no_T)\n",
        "for idx, v in enumerate(col_vals):\n",
        "  col_vals[idx] = \"T_to_\" + col_vals[idx]\n",
        "col_labels = dict(zip(col_keys, col_vals))\n",
        "dfDelta = dfDelta.rename(index=str, columns=col_labels)\n",
        "\n",
        "for v in col_vals:\n",
        "  dfDelta[v] = abs(dfDelta[v] - dfDelta[\"T\"])\n",
        "\n",
        "# add a row for sums and and delta and calculate\n",
        "\n",
        "# \n",
        "dfDelta.loc[\"sum\"] = dfDelta.sum(axis=0)\n",
        "dfDelta.loc[\"delta\"] = dfDelta.loc[\"sum\"]/len(features)\n",
        "\n",
        "# clean up stupid error\n",
        "all_col_labels = list(dfDelta.columns)\n",
        "for col in all_col_labels:\n",
        "  if col not in col_vals:\n",
        "    dfDelta.loc[\"sum\",col] = \"\"\n",
        "    dfDelta.loc[\"delta\", col] = \"\"\n",
        "\n",
        "dfDelta.tail(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lRGN-pryV6m"
      },
      "source": [
        "# report out\n",
        "\n",
        "for col in col_vals:\n",
        "  print(\"Delta for \" + col + \" is: \",dfDelta.loc[\"delta\",col])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY9YyB2tvoVp"
      },
      "source": [
        "This was the conclusion/outpt in PH\n",
        "```\n",
        "Delta score for candidate Hamilton is 1.768470453004334\n",
        "Delta score for candidate Madison is 1.6089724119682816\n",
        "Delta score for candidate Jay is 1.5345768956569326\n",
        "Delta score for candidate Disputed is 1.5371768107570636\n",
        "Delta score for candidate Shared is 1.846113566619675\n",
        "```\n",
        "Our calculations are similar but do not match precisely. Apparently, differences in text pre-processing?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt8sCDu3IEms"
      },
      "source": [
        "# Stylometry II PCA (Principal Component Analysis) Experiment\n",
        "Here what we are doing, is essentially repeating what we did when applying Burrow's Delta, with two important changes: (1) we are applying it to each publication separately; and (2) we are comparing every work to every other work to gauge \"closeness\" or \"distance.\"   \n",
        "As with topic models or word vector embeddings, we can think of each sample as a vector and use a distance measurement to position them in multidimensional space.\n",
        "Here there's a problem: how do we examine or visualize these relationships, and how do we find the important ones? (With 30 features, we need to allow up to 29 dimensions to describe the variation. Humans have difficulty imagining more than three.)   \n",
        "**Multi-Dimensional Scaling**, which we experimented with in connection with topic models, attempts to represent a best look into multi-dimensional space (the number of topics) in two (or three) dimensions, while trying to preserve the distances between data points. See 2D MDS on the location of countries on a globe in this tutorial: http://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/mds.html    \n",
        "Here we use a different technique called **Principal Component Analysis**. Unlike MDS, PCA attempts to calculate the directions of the dimensions and their weights. When we are looking at a two dimensional plot of principal components, we are looking at two specific dimensions calculated by the model. The analysis also can tell us how much of the variation among our test cases is explained by the specific components.   \n",
        "In our case, using the same number of features as before, the first two components account for a little under a quarter of the variation among the documents, but it looks like the first component (the horizontal or x axis) is where a good deal of the author differentiation is happening."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6dSr9dg8t1M"
      },
      "source": [
        "##Prepare the data\n",
        "Here we go over the steps that we followed for Burrow's delta to get a frequency table for each individual publication in the Federalist Papers.    \n",
        "Reformat in a transposed table for PCA. (Without transposing, PCA would calculate how much features differ from one another over the 85D space of the authors. A very different question.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLloEreA8wno"
      },
      "source": [
        "# Repeat earlier steps, but now asigning each paper not author to a colum \n",
        "# copy features and corpus data\n",
        "\n",
        "#repeating here allows us to experiment with feature values\n",
        "#separate into a list of values\n",
        "corpus_tokens = corpus.split()\n",
        "\n",
        "# create frequency list using built in nltk function\n",
        "whole_corpus_freq_dist = list(nltk.FreqDist(corpus_tokens).most_common(30))\n",
        "\n",
        "# # uncomment to see the check the first 10\n",
        "# whole_corpus_freq_dist[ :10 ]\n",
        "\n",
        "# data structure to contain our statistical information\n",
        "dfFeatures_pca = pd.DataFrame( columns=[\"feats\"])\n",
        "dfFeatures_pca[\"feats\"] = [w for w, freq in whole_corpus_freq_dist]\n",
        "dfFeatures_pca[\"corpus\"] = [freq for w, freq in whole_corpus_freq_dist]\n",
        "\n",
        "\n",
        "# create frequency table by publication rather than author\n",
        "for p in range(1,len(dfPapers)+1):  # iterate over all the files\n",
        "  paper_corpus = ' '.join(dfPapers[\"text\"][dfPapers[\"num\"] == p].values)\n",
        "\n",
        "  #separate into a list of values\n",
        "  paper_tokens = paper_corpus.split()\n",
        "  \n",
        "  # create frequency list using built in nltk function\n",
        "  paper_length = len(paper_tokens)\n",
        "\n",
        "  # copy the features to a list\n",
        "  # for each feature count the proportion of features to total author words\n",
        "  # append to df\n",
        "\n",
        "  features = dfFeatures_pca.feats.to_list()\n",
        "  # # raw numbers\n",
        "  # paper_features = [paper_tokens.count(x) for x in features]\n",
        "\n",
        "  # proportions\n",
        "  paper_features = [paper_tokens.count(x)/paper_length for x in features]\n",
        "  dfFeatures_pca[p] = paper_features\n",
        "\n",
        "\n",
        "# transpose\n",
        "dfFeats_transp = dfFeatures_pca.transpose()\n",
        "dfFeats_transp.drop([\"feats\", \"corpus\"],inplace=True)\n",
        "\n",
        "for f in features:\n",
        "  dfFeats_transp.rename(columns={features.index(f):f},inplace=True)\n",
        "\t\n",
        "\n",
        "# # uncomment to show check data\n",
        "# dfFeats_transp.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaVnZPyEcm3p"
      },
      "source": [
        "##Calcuate PCA \n",
        "We are using tools built into the scikit-learn library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsjFcX8Ocnf4"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "# # # to scale (mean = 0 total stdev = 1)\n",
        "# # # balances the weights of more q less frequent words\n",
        "# scaled = StandardScaler().fit_transform(dfFeats_transp)\n",
        "# principalComponents = pca.fit_transform(scaled)\n",
        "\n",
        "# to apply scaling uncomment above and comment next line\n",
        "principalComponents = pca.fit_transform(dfFeats_transp)\n",
        "principalDf = pd.DataFrame(data = principalComponents,\n",
        "                           columns = ['principal component 1', \n",
        "                           'principal component 2'])\n",
        "\n",
        "# add attrib labels\n",
        "principalDf[\"attrib\"] = dfPapers[\"attrib\"]\n",
        "\n",
        "principalDf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXJybJG4g5NG"
      },
      "source": [
        "## Plot a 2-Dimensional Grid for two Components.\n",
        "Plot a 2D scatter chart for the two first principal components.  \n",
        "Federalist 64 is marked in red on the plot. Does this confirm our earlier obsevation using Burrows's Delta?  \n",
        "This was Laramée's concluding paragraph:\n",
        ">As expected, Delta identifies John Jay as Federalist 64’s most likely author. It is interesting to note that, according to Delta, Federalist 64 is more similar to the disputed papers than to those known to have been written by Hamilton or by Madison; why that might be, however, is a question for another day.   \n",
        "\n",
        "Is this still true?  \n",
        "What happens when we increase the number of features?\n",
        "Decrease?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmuHWbJdg8Dp"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "## plotting code from:\n",
        "## https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
        "\n",
        "# print out the amount explained by the first two components\n",
        "print(pca.explained_variance_ratio_)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = [\"H\",\"M\",\"J\",\"S\",\"D\",\"T\"]\n",
        "colors = ['purple', 'green', 'black','yellow', 'blue', 'red']\n",
        "for target, color in zip(targets,colors):\n",
        "  indicesToKeep = principalDf['attrib'] == target\n",
        "  ax.scatter(principalDf.loc[indicesToKeep, 'principal component 1']\n",
        "               , principalDf.loc[indicesToKeep, 'principal component 2']\n",
        "               , c = color\n",
        "               , s = 50)\n",
        "ax.legend(targets)\n",
        "ax.grid()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}